{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>776522983837954049</td>\n",
       "      <td>735449229028675584</td>\n",
       "      <td>2016-09-15 20:48:01</td>\n",
       "      <td>se lo dici tu... https://t.co/x7Qm1VHBKL</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>51c0e6b24c64e54e</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>46.0027</td>\n",
       "      <td>8.96044</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>plvtone filiae.</td>\n",
       "      <td>hazel_chb</td>\n",
       "      <td>146</td>\n",
       "      <td>110.0</td>\n",
       "      <td>28621.0</td>\n",
       "      <td>Earleen.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                   1                    2   \\\n",
       "0  776522983837954049  735449229028675584  2016-09-15 20:48:01   \n",
       "\n",
       "                                         3   4   5                 6   7   \\\n",
       "0  se lo dici tu... https://t.co/x7Qm1VHBKL  \\N  \\N  51c0e6b24c64e54e  \\N   \n",
       "\n",
       "    8  9        10       11                  12  \\\n",
       "0  1.0  \u0000  46.0027  8.96044  Twitter for iPhone   \n",
       "\n",
       "                                      13               14         15   16  \\\n",
       "0  http://twitter.com/#!/download/iphone  plvtone filiae.  hazel_chb  146   \n",
       "\n",
       "      17       18        19  \n",
       "0  110.0  28621.0  Earleen.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample = pd.read_excel('Data\\sample.tsv.xlsx',header = None)\n",
    "sample.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from imp import reload\n",
    "sys.path.insert(0, \"/media/diskD/EPFL/Fall 2016/ADA/Project/GMR_ADA_Project/EmotionAnalysis\")\n",
    "#from DataSchemaExtractionParsing import *\n",
    "from DataPreProcessing import *\n",
    "#from SentSemanticModule import *\n",
    "#from SentTweetModule import *\n",
    "#from SentSyntacticModule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, h = 8, 2\n",
    "matrix_sentence = [[0 for x in range(w)] for y in range(h)]\n",
    "matrix_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lexicon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ce7054e21ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_emotional_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_emotional_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_emotional_vectors_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_emotional_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lexicon' is not defined"
     ]
    }
   ],
   "source": [
    "word_set = list(set(lexicon['Word']))\n",
    "word_emotional_vectors = []\n",
    "for word in word_set:\n",
    "    word_emotional_vectors.append((word,list(lexicon[lexicon['Word']==word]['Score'])))\n",
    "word_emotional_vectors_dict = dict(word_emotional_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lexicon = pd.read_csv('NRCLexicon/NRC-Emotion-Lexicon-v0.92/NRC-emotion-lexicon.txt',engine='python', names=['col'])\n",
    "lexicon['Word'] = lexicon['col'].str.split().apply(lambda x: x[0])\n",
    "lexicon['EmotionCategory'] = lexicon['col'].str.split().apply(lambda x: x[1])\n",
    "lexicon['Score'] = lexicon['col'].str.split().apply(lambda x: x[-1])\n",
    "del(lexicon['EmotionCategory'])\n",
    "del(lexicon['col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 14182/14182 [04:36<00:00, 50.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "distinct_words = list(set(lexicon['Word']))\n",
    "word_vectors = []\n",
    "for i in tqdm(range(0,len(distinct_words))):\n",
    "    vector = list(lexicon[lexicon['Word']==distinct_words[i]]['Score'])\n",
    "    word_vectors.append((distinct_words[i],vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3b5ab213bed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mu'clap'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '0', '0', '1', '0', '1', '0', '0', '1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_dict = dict(word_vectors)\n",
    "vector_dict['happy']\n",
    "import numpy as np\n",
    "#np.save('NRCLexicon/lexicon_dictionary.npy', vector_dict)\n",
    "if 'aims' in vector_dict.keys():\n",
    "    print (\"yes\")\n",
    "list_ = [0 for i in range(0,10)]\n",
    "list_\n",
    "vector_dict['clap']\n",
    "\n",
    "lexicon_dict = np.load('NRCLexicon/lexicon_dictionary.npy').item()\n",
    "lexicon_dict['clap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_tweets = extract_tweets('Data/sample.tsv.xlsx')\n",
    "# Keeping Meaningful columns\n",
    "sample_tweets = sample_tweets[['createdAt', 'text', 'longitude', 'latitude', 'placeLongitude', 'placeLatitude', 'userLocation']]\n",
    "sample_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(sample_tweets['userLocation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_tweets_cleaned = cleanTweets(sample_tweets)\n",
    "sample_tweets_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_tweets = sample_tweets.dropna()\n",
    "#sample_tweets = sample_tweets.drop(sample_tweets.index[[2891]]).reindex()\n",
    "#sample_tweets = sample_tweets.drop(sample_tweets.index[[2890]]).reindex()\n",
    "#sample_tweets = sample_tweets.drop(sample_tweets.index[[2891]]).reindex() \n",
    "#sample_tweets = sample_tweets.drop(sample_tweets.index[[3916]]).reindex() \n",
    "#sample_tweets = sample_tweets.drop(sample_tweets.index[[4311]]).reindex()\n",
    "#sample_tweets = sample_tweets.drop(sample_tweets.index[[7599]]).reindex()\n",
    "sample_tweets.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_tweets.iloc[864]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing and Cleaning Tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy:\n",
    "1. Parsing and Character Encoding\n",
    "2. Language Detection (Translation)\n",
    "3. Remove Tweets for which geolocation (longitude  is not present or where geolocation does not correspond to Switzerland\n",
    "4. Handling Entities/ Special categories: \n",
    "    4.1. Replacing @ instances with <username>\n",
    "    4.2. Replacing urls with <url>\n",
    "    4.3. Replacing Emoticons with their word meaning\n",
    "    4.4. Replacing numbers/phone/fax with <number>\n",
    "    4.5. Detecting place / city / country / any geolocation cues in any part of the tweet (#)\n",
    "    4.6. Detecting time cues\n",
    "5. Tokenization and replacing contractions\n",
    "6. Part of Speech Tagging to recognize Affective words (Noun, Verbs, Adjectives, Adverb) \n",
    "7. Some NRE to replace basic entities like Proper Nouns with tag <proper_noun>\n",
    "8. Stopwords and punctuation removal\n",
    "9. (possibly spell-checking as well)\n",
    "10. Lowering multiple occurences of a character in a word (words like soooooo => so)\n",
    "11. Lemmatization and term normalization to get less variable versions of the same word. (possibly use thesaurus also) \n",
    "12. Remove less frequent words => word count + define a specific threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Language Detection & Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yandex_translate import YandexTranslate\n",
    "translate = YandexTranslate('trnsl.1.1.20161128T210956Z.66d67a07332e46b7.66bee9bcc58c60428596f3e559b4a75a1944f839')\n",
    "\n",
    "translated_list = []\n",
    "for i in range(0,len(sample_tweets)):\n",
    "    print i\n",
    "    translated_list.append(translate.translate(sample_tweets.iloc[i]['text'], 'en')['text'])\n",
    "sample_tweets['EnglishText'] = translated_list\n",
    "sample_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Tweets for which geolocation is not present or where geolocation does not correspond to Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handling Special Entities / Categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Replacing URLs in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of Tweets into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "word_tokenize(sample_tweets['text'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
