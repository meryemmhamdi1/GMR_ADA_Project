{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, \"/media/diskD/EPFL/Fall 2016/ADA/Project/GMR_ADA_Project/EmotionAnalysis\")\n",
    "from DataSchemaExtractionParsing import *\n",
    "from DataPreProcessing import *\n",
    "#from SentSemanticModule import *\n",
    "from SentTweetModule import *\n",
    "from SentSyntacticModule import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Annotated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How much of the forecast was genuine and how m...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I did touch them one time you see but of cours...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We find that choice theorists admit that they ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well, here I am with an olive branch.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Its rudder and fin were both knocked out, and ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion\n",
       "0  How much of the forecast was genuine and how m...  Neutral\n",
       "1  I did touch them one time you see but of cours...  Neutral\n",
       "2  We find that choice theorists admit that they ...  Neutral\n",
       "3              Well, here I am with an olive branch.  Neutral\n",
       "4  Its rudder and fin were both knocked out, and ...  Neutral"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plutchik_tweets = pd.read_csv('Data/primary-plutchik-wheel-DFE.csv',encoding=\"utf-8\")\n",
    "plutchik_tweets = plutchik_tweets[['text','emotion']]\n",
    "plutchik_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2524"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plutchik_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Aggression',\n",
       " u'Ambiguous',\n",
       " u'Anger',\n",
       " u'Anticipation',\n",
       " u'Awe',\n",
       " u'Contempt',\n",
       " u'Disapproval',\n",
       " u'Disgust',\n",
       " u'Fear',\n",
       " u'Joy',\n",
       " u'Love',\n",
       " u'Neutral',\n",
       " u'Optimism',\n",
       " u'Remorse',\n",
       " u'Sadness',\n",
       " u'Submission',\n",
       " u'Surprise',\n",
       " u'Trust'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(plutchik_tweets['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "949"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selective_8 = ['Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust','Neutra']\n",
    "plutchik_tweets_8emotions = plutchik_tweets[plutchik_tweets['emotion'].isin(selective_8)]\n",
    "len(plutchik_tweets_8emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the same pre-processing pipeline to get affective representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_no_contractions = replace_contractions(plutchik_tweets_8emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of Sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_list = bag_of_word_representation(tweets_no_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_tweets = pos_tagging(tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import nsubj, advmod, acomp, amod, neg, NOUN, VERB, ADJ, ADV\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "# Joining text:\n",
    "tweets_text = []\n",
    "for i in range(0, len(tokenized_list)):\n",
    "    space = \" \"\n",
    "    tweets_text.append(space.join(tokenized_list[i]))\n",
    "tweets_text[0].encode(\"utf-8\")\n",
    "for i in range(0, len(tweets_text)):\n",
    "    doc = nlp(tweets_text[i])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples = []\n",
    "for sample in docs:\n",
    "    new_samples_sub = []\n",
    "    for word in sample:\n",
    "        new_samples_sub.append((unicode(word),word.pos_))\n",
    "    new_samples.append(new_samples_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Syntactic Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<inside advmod>>>\n",
      "[(u'Brian', u'PROPN'), (u'as', u'ADP'), (u'decided', u'VERB'), (u'not', u'ADV'), (u'to', u'PART'), (u'sit', u'VERB'), (u'on', u'ADP'), (u'the', u'DET'), (u'fence', u'NOUN')]\n",
      "<<<<<<<<<inside negation>>>>>>>>>\n",
      "[(u'Brian', u'PROPN'), (u'as', u'ADP'), (u'decided', u'VERB'), (u'to', u'PART'), (u'on', u'ADP'), (u'the', u'DET'), (u'fence', u'NOUN'), ('not_sit', u'VERB')]\n",
      "(u'sit', u'VERB')\n",
      "(u'not', u'ADV')\n",
      "<<<<<<<inside verb\n",
      "[(u'Brian', u'PROPN'), (u'as', u'ADP'), (u'decided', u'VERB'), (u'to', u'PART'), (u'on', u'ADP'), (u'the', u'DET'), (u'fence', u'NOUN'), ('not_sit', u'VERB')]\n",
      "<<<<<<<inside negation noun>>>\n",
      "[(u'Brian', u'PROPN'), (u'as', u'ADP'), (u'decided', u'VERB'), (u'to', u'PART'), (u'on', u'ADP'), (u'the', u'DET'), (u'fence', u'NOUN'), ('not_sit', u'VERB')]\n"
     ]
    }
   ],
   "source": [
    "triple_dependencies = []\n",
    "samples = docs\n",
    "for i in range(0,len(samples)):\n",
    "    negated_verbs = []\n",
    "    negated_nouns = []\n",
    "    triple_dependencies_sub = []\n",
    "    for j in range(0,len(samples[i])):\n",
    "        #print \"i=\"+str(i)\n",
    "        #print \"j=\"+str(j)\n",
    "        word = samples[i][j]\n",
    "        #print word\n",
    "        triple_dependencies_sub.append((word,word.dep_,word.head))\n",
    "        # Looking for negation dependency\n",
    "        if word.dep == neg:\n",
    "            new_samples[i].append((\"not_\"+str(word.head),word.head.pos_))\n",
    "            new_samples[i] = [x for x in new_samples[i] if x != (unicode(word.head),word.head.pos_) and x!=(unicode(word),word.pos_)]\n",
    "            if word.head.pos == VERB:\n",
    "                negated_verbs.append(word.head.i)\n",
    "        # Looking for adjectival complement\n",
    "        if word.dep == acomp:\n",
    "            \n",
    "            if word.head.i in negated_verbs:\n",
    "                new_samples[i].append((\"not_\"+str(word),word.pos_))\n",
    "                new_samples[i] = [x for x in new_samples[i] if x != (\"not_\"+str(word.head),word.head.pos_) and x != (unicode(word),word.pos_)]\n",
    "            else: \n",
    "                new_samples[i] = [x for x in new_samples[i] if x != (unicode(word.head),word.head.pos_)]\n",
    "        # Looking for negation in nouns and adjectives\n",
    "        if word.lemma_ == \"no\" or word.lemma_ == 'not' or word.lemma_ == \"never\":\n",
    "            if word.head.pos == NOUN:\n",
    "                new_samples[i].append((\"not_\"+str(word.head),word.head.pos_))\n",
    "                new_samples[i] = [x for x in new_samples[i] if x != (unicode(word.head),word.head.pos_) and x!=(unicode(word),word.pos_)]\n",
    "                negated_nouns.append(word.head.i)\n",
    "        # Looking for adjectival modifier \n",
    "        if word.dep == amod:\n",
    "            if word.head.i in negated_nouns:\n",
    "                new_samples[i].append((\"not_\"+str(word),word.pos_))\n",
    "                new_samples[i] = [x for x in new_samples[i] if x != (\"not_\"+str(word.head),word.head.pos_) and x != (unicode(word),word.pos_)]\n",
    "            else:\n",
    "                new_samples[i] = [x for x in new_samples[i] if x != (unicode(word.head),word.head.pos_)]\n",
    "        # Looking for adverbial modifier\n",
    "        if word.dep == advmod:\n",
    "            new_samples[i] = [x for x in new_samples[i] if x != (unicode(word),word.pos_)]\n",
    "    triple_dependencies.append(triple_dependencies_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<< Original tweet text >>>\n",
      "\n",
      "Brian as ever decided not to sit on the fence\n",
      "\n",
      "<<<< Syntactic dependencies >>>\n",
      "\n",
      "[(Brian, u'nsubj', decided), (as, u'mark', decided), (ever, u'advmod', decided), (decided, u'ROOT', decided), (not, u'neg', sit), (to, u'aux', sit), (sit, u'xcomp', decided), (on, u'prep', sit), (the, u'det', fence), (fence, u'pobj', on)]\n",
      "\n",
      "<<<< Tweet after applying syntactic Rules >>>\n",
      "\n",
      "[u'Brian', u'as', u'decided', u'to', u'on', u'the', u'fence', 'not_sit']\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print \"\\n<<<< Original tweet text >>>\\n\"\n",
    "print tweets_text[i]\n",
    "print \"\\n<<<< Syntactic dependencies >>>\\n\"\n",
    "print triple_dependencies[i]\n",
    "print \"\\n<<<< Tweet after applying syntactic Rules >>>\\n\"\n",
    "#print new_samples[i]\n",
    "new_tweet = []\n",
    "for (word,pos) in new_samples[i]:\n",
    "    new_tweet.append(word)\n",
    "print new_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    non_entity_names = []\n",
    "    entity_names = []\n",
    "   \n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "    else:\n",
    "        non_entity_names.append(t)\n",
    "    return non_entity_names\n",
    "\n",
    "tweet_without_ne = []\n",
    "for tweet in new_samples:\n",
    "    nre_tweet = nltk.ne_chunk(tweet, binary = True)\n",
    "    non_entity_names = []\n",
    "    for tree in nre_tweet:    \n",
    "        non_entity_names.extend(extract_entity_names(tree))\n",
    "    tweet_without_ne.append(non_entity_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'as', u'ADP'),\n",
       " (u'decided', 'v'),\n",
       " (u'to', u'PART'),\n",
       " (u'on', u'ADP'),\n",
       " (u'the', u'DET'),\n",
       " (u'fence', 'n'),\n",
       " ('not_sit', 'v')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_pos_tags_words(tagged_tweets):\n",
    "    \"\"\"\n",
    "    :param tagged_tweets:\n",
    "    :return: tweets_nava\n",
    "    \"\"\"\n",
    "    tweets_nava = []\n",
    "    tweets_nava_sub = []\n",
    "    for i in range(0, len(tagged_tweets)):\n",
    "        for (word, tag) in tagged_tweets[i]:\n",
    "            if tag == \"NOUN\":\n",
    "                tweets_nava_sub.append((word, 'n'))\n",
    "            elif tag == 'VERB':\n",
    "                tweets_nava_sub.append((word, 'v'))\n",
    "            else: \n",
    "                tweets_nava_sub.append((word, tag))\n",
    "        tweets_nava.append(list(tweets_nava_sub))\n",
    "        tweets_nava_sub = []\n",
    "    return tweets_nava\n",
    "normalized_tags = normalize_pos_tags_words(tweet_without_ne)\n",
    "normalized_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation and Stop words and Converting to Lower Case and Removal of Other special categories: url, number, username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def eliminate_stop_words_punct(tagged_tweets):\n",
    "    \"\"\"\n",
    "    Elimination of Stop words\n",
    "    Elimination of Punctuation\n",
    "\n",
    "    :rtype: object\n",
    "    :param tagged_tweets:\n",
    "    :return: tagged_tweets_without\n",
    "    \"\"\"\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    tagged_tweets_without = []\n",
    "    for i in range(0, len(tagged_tweets)):\n",
    "        tagged_tweets_without_sub = []\n",
    "        for (word, tag) in tagged_tweets[i]:\n",
    "            if word not in stop_words and word not in ['url','number','username'] and len(word) >= 2:\n",
    "                tagged_tweets_without_sub.append((word.lower(), tag))\n",
    "        tagged_tweets_without.append(tagged_tweets_without_sub)\n",
    "    return tagged_tweets_without\n",
    "tagged_tweets_without = eliminate_stop_words_punct(normalized_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'over', u'ADP'), (u'dead', u'ADJ'), (u'arrest', 'v')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizer(tweets):\n",
    "    tweets_whole = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweets_sub = []\n",
    "        for (word,tag) in tweets[i]:\n",
    "            if tag=='v' or tag =='n':\n",
    "                tweets_sub.append((lmtzr.lemmatize(word,tag),tag))\n",
    "            else: \n",
    "                tweets_sub.append((word,tag))\n",
    "        tweets_whole.append(tweets_sub)\n",
    "    return tweets_whole\n",
    "\n",
    "lemmatized_tweets = lemmatizer(tagged_tweets_without)\n",
    "\n",
    "\n",
    "def lemmatizer_untagged(tweets):\n",
    "    tweets_whole = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweets_sub = []\n",
    "        for (word,tag) in tweets[i]:\n",
    "            if tag=='v' or tag =='n':\n",
    "                tweets_sub.append(lmtzr.lemmatize(word,tag))\n",
    "            else: \n",
    "                tweets_sub.append(word)\n",
    "        tweets_whole.append(tweets_sub)\n",
    "    return tweets_whole\n",
    "lemmatized_tweets_untag = lemmatizer_untagged(tagged_tweets_without)\n",
    "lemmatized_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only NAVA words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_only_nava_words(tagged_tweets):\n",
    "    \"\"\"\n",
    "    :param tagged_tweets:\n",
    "    :return: tweets_nava\n",
    "    \"\"\"\n",
    "    tweets_nava = []\n",
    "    tweets_nava_sub = []\n",
    "    for i in range(0, len(tagged_tweets)):\n",
    "        for (word, tag) in tagged_tweets[i]:\n",
    "            if tag == \"n\" or tag == \"v\" or tag ==\"ADJ\" or tag == \"ADV\":\n",
    "                tweets_nava_sub.append(word)\n",
    "        tweets_nava.append(list(tweets_nava_sub))\n",
    "        tweets_nava_sub = []\n",
    "    return tweets_nava\n",
    "nava_tweets = keep_only_nava_words(lemmatized_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'over', u'my', u'dead', u'body', u'are', u'you', u'arresting', u'him']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizer_raw(tweets):\n",
    "    tweets_whole = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for i in range(0,len(tweets)):\n",
    "        tweets_sub = []\n",
    "        for (word,tag) in tweets[i]:\n",
    "            if tag=='v' or tag =='n':\n",
    "                tweets_sub.append(lmtzr.lemmatize(word,tag).lower())\n",
    "            else: \n",
    "                tweets_sub.append(word.lower())\n",
    "        tweets_whole.append(tweets_sub)\n",
    "    return tweets_whole\n",
    "raw_tokenized_lemma = lemmatizer_raw(tagged_tweets)\n",
    "len(raw_tokenized_lemma)\n",
    "raw_tokenized_lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(raw_tokenized_lemma, workers=num_workers, \n",
    "            size=num_features, min_count = min_word_count, \n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"external_tweets_word2vec_model\"\n",
    "model.save(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'arresting' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-f56797dc2fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arresting'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/meryem/miniconda2/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'arresting' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar('arresting')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
