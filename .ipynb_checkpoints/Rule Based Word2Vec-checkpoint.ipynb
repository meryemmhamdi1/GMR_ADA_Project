{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model for Multiple Languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 331035/331035 [00:16<00:00, 19806.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# LOADING TOKENIZED REPRESENTATION\n",
    "import pandas as pd\n",
    "tokenized = pd.read_csv('../../Project_Backup/BigData/Unannotated_Representation/de/Unannotated_Representation1_sw.csv',encoding=\"ISO-8859-1\")\n",
    "tokenized1 = pd.read_csv('../../Project_Backup/BigData/Unannotated_Representation/de/Unannotated_Representation2_sw.csv',encoding=\"ISO-8859-1\")\n",
    "tokenized2 = pd.read_csv('../../Project_Backup/BigData/Unannotated_Representation/de/Unannotated_Representation3_sw.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "tokenized_lemma = tokenized['Tokenized Lemmatized']\n",
    "tokenized_lemmatized_tweets = []\n",
    "for i in tqdm(range(0, len(tokenized_lemma))):\n",
    "    result = ast.literal_eval(tokenized_lemma[i])\n",
    "    tokenized_lemmatized_tweets.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-29 16:11:15,372 : INFO : collecting all words and their counts\n",
      "2017-01-29 16:11:15,372 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-29 16:11:15,403 : INFO : PROGRESS: at sentence #10000, processed 84441 words, keeping 29071 word types\n",
      "2017-01-29 16:11:15,434 : INFO : PROGRESS: at sentence #20000, processed 170345 words, keeping 49332 word types\n",
      "2017-01-29 16:11:15,481 : INFO : PROGRESS: at sentence #30000, processed 255844 words, keeping 65974 word types\n",
      "2017-01-29 16:11:15,528 : INFO : PROGRESS: at sentence #40000, processed 338568 words, keeping 80257 word types\n",
      "2017-01-29 16:11:15,559 : INFO : PROGRESS: at sentence #50000, processed 413664 words, keeping 91880 word types\n",
      "2017-01-29 16:11:15,590 : INFO : PROGRESS: at sentence #60000, processed 493851 words, keeping 104104 word types\n",
      "2017-01-29 16:11:15,622 : INFO : PROGRESS: at sentence #70000, processed 569217 words, keeping 115007 word types\n",
      "2017-01-29 16:11:15,668 : INFO : PROGRESS: at sentence #80000, processed 641917 words, keeping 124531 word types\n",
      "2017-01-29 16:11:15,700 : INFO : PROGRESS: at sentence #90000, processed 726330 words, keeping 135634 word types\n",
      "2017-01-29 16:11:15,747 : INFO : PROGRESS: at sentence #100000, processed 811680 words, keeping 146954 word types\n",
      "2017-01-29 16:11:15,778 : INFO : PROGRESS: at sentence #110000, processed 887266 words, keeping 156847 word types\n",
      "2017-01-29 16:11:15,825 : INFO : PROGRESS: at sentence #120000, processed 967969 words, keeping 167860 word types\n",
      "2017-01-29 16:11:15,872 : INFO : PROGRESS: at sentence #130000, processed 1052519 words, keeping 178529 word types\n",
      "2017-01-29 16:11:15,903 : INFO : PROGRESS: at sentence #140000, processed 1130331 words, keeping 187697 word types\n",
      "2017-01-29 16:11:15,950 : INFO : PROGRESS: at sentence #150000, processed 1206086 words, keeping 196599 word types\n",
      "2017-01-29 16:11:15,983 : INFO : PROGRESS: at sentence #160000, processed 1294704 words, keeping 206904 word types\n",
      "2017-01-29 16:11:16,029 : INFO : PROGRESS: at sentence #170000, processed 1377777 words, keeping 216292 word types\n",
      "2017-01-29 16:11:16,061 : INFO : PROGRESS: at sentence #180000, processed 1454209 words, keeping 224484 word types\n",
      "2017-01-29 16:11:16,108 : INFO : PROGRESS: at sentence #190000, processed 1535393 words, keeping 233401 word types\n",
      "2017-01-29 16:11:16,154 : INFO : PROGRESS: at sentence #200000, processed 1625638 words, keeping 242529 word types\n",
      "2017-01-29 16:11:16,186 : INFO : PROGRESS: at sentence #210000, processed 1719805 words, keeping 252223 word types\n",
      "2017-01-29 16:11:16,233 : INFO : PROGRESS: at sentence #220000, processed 1813741 words, keeping 260431 word types\n",
      "2017-01-29 16:11:16,279 : INFO : PROGRESS: at sentence #230000, processed 1909318 words, keeping 269166 word types\n",
      "2017-01-29 16:11:16,311 : INFO : PROGRESS: at sentence #240000, processed 2004445 words, keeping 277699 word types\n",
      "2017-01-29 16:11:16,358 : INFO : PROGRESS: at sentence #250000, processed 2100055 words, keeping 286767 word types\n",
      "2017-01-29 16:11:16,404 : INFO : PROGRESS: at sentence #260000, processed 2196172 words, keeping 295917 word types\n",
      "2017-01-29 16:11:16,467 : INFO : PROGRESS: at sentence #270000, processed 2294649 words, keeping 304386 word types\n",
      "2017-01-29 16:11:16,514 : INFO : PROGRESS: at sentence #280000, processed 2381548 words, keeping 312168 word types\n",
      "2017-01-29 16:11:16,561 : INFO : PROGRESS: at sentence #290000, processed 2472827 words, keeping 319536 word types\n",
      "2017-01-29 16:11:16,608 : INFO : PROGRESS: at sentence #300000, processed 2565389 words, keeping 327285 word types\n",
      "2017-01-29 16:11:16,639 : INFO : PROGRESS: at sentence #310000, processed 2658002 words, keeping 334575 word types\n",
      "2017-01-29 16:11:16,686 : INFO : PROGRESS: at sentence #320000, processed 2748329 words, keeping 341537 word types\n",
      "2017-01-29 16:11:16,733 : INFO : PROGRESS: at sentence #330000, processed 2840271 words, keeping 348449 word types\n",
      "2017-01-29 16:11:16,748 : INFO : collected 349206 word types from a corpus of 2849222 raw words and 331035 sentences\n",
      "2017-01-29 16:11:16,748 : INFO : Loading a fresh vocabulary\n",
      "2017-01-29 16:11:17,634 : INFO : min_count=1 retains 349206 unique words (100% of original 349206, drops 0)\n",
      "2017-01-29 16:11:17,634 : INFO : min_count=1 leaves 2849222 word corpus (100% of original 2849222, drops 0)\n",
      "2017-01-29 16:11:18,853 : INFO : deleting the raw counts dictionary of 349206 items\n",
      "2017-01-29 16:11:18,884 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2017-01-29 16:11:18,884 : INFO : downsampling leaves estimated 2740492 word corpus (96.2% of prior 2849222)\n",
      "2017-01-29 16:11:18,884 : INFO : estimated required memory for 349206 words and 300 dimensions: 1012697400 bytes\n",
      "2017-01-29 16:11:20,234 : INFO : resetting layer weights\n",
      "2017-01-29 16:11:26,180 : INFO : training model with 4 workers on 349206 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-01-29 16:11:26,180 : INFO : expecting 331035 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-29 16:11:28,080 : INFO : PROGRESS: at 0.08% examples, 7642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:29,124 : INFO : PROGRESS: at 1.20% examples, 70510 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:30,164 : INFO : PROGRESS: at 3.67% examples, 143284 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:31,261 : INFO : PROGRESS: at 6.10% examples, 177366 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:32,307 : INFO : PROGRESS: at 8.54% examples, 199823 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:33,374 : INFO : PROGRESS: at 10.76% examples, 211529 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:34,381 : INFO : PROGRESS: at 12.94% examples, 223842 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:35,392 : INFO : PROGRESS: at 15.49% examples, 242071 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:36,404 : INFO : PROGRESS: at 18.20% examples, 258856 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:37,423 : INFO : PROGRESS: at 20.86% examples, 269425 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:38,475 : INFO : PROGRESS: at 23.37% examples, 272870 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:39,496 : INFO : PROGRESS: at 25.86% examples, 276049 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:40,492 : INFO : PROGRESS: at 28.85% examples, 283238 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:41,502 : INFO : PROGRESS: at 31.77% examples, 290056 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:42,539 : INFO : PROGRESS: at 34.41% examples, 295955 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:43,545 : INFO : PROGRESS: at 36.69% examples, 298946 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:44,566 : INFO : PROGRESS: at 39.19% examples, 302273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:45,585 : INFO : PROGRESS: at 41.47% examples, 302789 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:46,599 : INFO : PROGRESS: at 44.22% examples, 304618 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:47,617 : INFO : PROGRESS: at 47.03% examples, 306829 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:48,644 : INFO : PROGRESS: at 49.69% examples, 308459 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:49,643 : INFO : PROGRESS: at 52.42% examples, 310853 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:50,670 : INFO : PROGRESS: at 55.09% examples, 314343 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:51,694 : INFO : PROGRESS: at 57.55% examples, 316230 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:52,702 : INFO : PROGRESS: at 60.06% examples, 317860 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:53,752 : INFO : PROGRESS: at 62.57% examples, 317932 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:54,797 : INFO : PROGRESS: at 65.71% examples, 320259 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:55,817 : INFO : PROGRESS: at 68.92% examples, 322912 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:56,838 : INFO : PROGRESS: at 71.90% examples, 325073 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:11:57,874 : INFO : PROGRESS: at 74.64% examples, 327481 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:58,909 : INFO : PROGRESS: at 76.88% examples, 327532 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:11:59,923 : INFO : PROGRESS: at 79.05% examples, 327062 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:12:00,926 : INFO : PROGRESS: at 81.53% examples, 327573 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:12:01,940 : INFO : PROGRESS: at 84.79% examples, 329616 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:12:02,949 : INFO : PROGRESS: at 87.74% examples, 331061 words/s, in_qsize 8, out_qsize 1\n",
      "2017-01-29 16:12:03,991 : INFO : PROGRESS: at 91.04% examples, 333186 words/s, in_qsize 7, out_qsize 1\n",
      "2017-01-29 16:12:05,003 : INFO : PROGRESS: at 93.62% examples, 334157 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:12:06,010 : INFO : PROGRESS: at 96.26% examples, 335882 words/s, in_qsize 8, out_qsize 0\n",
      "2017-01-29 16:12:07,010 : INFO : PROGRESS: at 99.04% examples, 337552 words/s, in_qsize 7, out_qsize 0\n",
      "2017-01-29 16:12:07,294 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-29 16:12:07,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-29 16:12:07,323 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-29 16:12:07,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-29 16:12:07,353 : INFO : training on 14246110 raw words (13702464 effective words) took 40.5s, 338104 effective words/s\n",
      "2017-01-29 16:12:08,021 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-01-29 16:12:10,816 : INFO : saving Word2Vec object under ../../Project_Backup/BigData/Models/whole_tr_model, separately None\n",
      "2017-01-29 16:12:10,818 : INFO : storing np array 'syn0' to ../../Project_Backup/BigData/Models/whole_tr_model.wv.syn0.npy\n",
      "2017-01-29 16:12:15,506 : INFO : not storing attribute cum_table\n",
      "2017-01-29 16:12:15,508 : INFO : storing np array 'syn1neg' to ../../Project_Backup/BigData/Models/whole_tr_model.syn1neg.npy\n",
      "2017-01-29 16:12:20,655 : INFO : not storing attribute syn0norm\n",
      "2017-01-29 16:12:21,992 : INFO : saved ../../Project_Backup/BigData/Models/whole_tr_model\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS TO BE TUNED:\n",
    "\n",
    "# Word vector dimensionality                      \n",
    "# Minimum word count                        \n",
    "# Number of threads to run in parallel\n",
    "# Context window size                                                                                    \n",
    "# Downsample setting for frequent words\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 1  # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(tokenized_lemmatized_tweets, workers=num_workers,\n",
    "            size=num_features, min_count = min_word_count,\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"../../Project_Backup/BigData/Models/whole_tr_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from EmotionAnalysis.SentSemanticModule import *\n",
    "from EmotionAnalysis.SentTweetModule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['hb']\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tokenized = pd.read_csv('../../Project_Backup/BigData/Unannotated_Representation/tr/Unannotated_Representation1_sw.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "nava_repr = list(tokenized['Nava without Stop Words'])\n",
    "nava_repr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert nava tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 331035/331035 [00:09<00:00, 36335.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hb']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Convert nava tweets\")\n",
    "# Convert nava_tweets \n",
    "nava_tweets = []\n",
    "for i in tqdm(range(0, len(nava_repr))):\n",
    "    result = ast.literal_eval(nava_repr[i])\n",
    "    nava_tweets.append(result)\n",
    "    #nava_tweets.append(nava_repr[i][1:len(nava_repr[i])-1].split(', '))\n",
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lexicon\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print (\"Loading Lexicon\")\n",
    "with open('NRCLexicon/turkish_lexicon.pickle', 'rb') as handle:\n",
    "    lexicon_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "representative_set = []\n",
    "for i in range(0,10):\n",
    "    representative_set_sub = []\n",
    "    for word in lexicon_dict.keys():\n",
    "        if lexicon_dict[word][i] == 1: \n",
    "            representative_set_sub.append(word)\n",
    "    representative_set.append(representative_set_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon_df = pd.DataFrame()\n",
    "lexicon_df[0] = representative_set[0]\n",
    "for i in range(1,10):\n",
    "    df = pd.DataFrame()\n",
    "    df[i] = representative_set[i]\n",
    "    lexicon_df= pd.concat([lexicon_df,df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kayıtsızlık</td>\n",
       "      <td>forewarned</td>\n",
       "      <td>kayıtsızlık</td>\n",
       "      <td>kayıtsızlık</td>\n",
       "      <td>faydalı</td>\n",
       "      <td>kayıtsızlık</td>\n",
       "      <td>üretici</td>\n",
       "      <td>kayıtsızlık</td>\n",
       "      <td>ortaya çıkarmak</td>\n",
       "      <td>haysiyet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aptallık</td>\n",
       "      <td>özgürlük</td>\n",
       "      <td>sapık</td>\n",
       "      <td>Alarm</td>\n",
       "      <td>yeşil</td>\n",
       "      <td>zoraki</td>\n",
       "      <td>haysiyet</td>\n",
       "      <td>aptallık</td>\n",
       "      <td>mistik</td>\n",
       "      <td>dayanak noktası</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tütsü</td>\n",
       "      <td>macera</td>\n",
       "      <td>aptallık</td>\n",
       "      <td>hoşlanmama</td>\n",
       "      <td>özgürlük</td>\n",
       "      <td>sahipsiz</td>\n",
       "      <td>dayanak noktası</td>\n",
       "      <td>hariç</td>\n",
       "      <td>Alarm</td>\n",
       "      <td>faydalı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hoşlanmama</td>\n",
       "      <td>far</td>\n",
       "      <td>hoşlanmama</td>\n",
       "      <td>şanssızlık</td>\n",
       "      <td>muzaffer</td>\n",
       "      <td>meme</td>\n",
       "      <td>faydalı</td>\n",
       "      <td>körü körüne</td>\n",
       "      <td>özgürlük</td>\n",
       "      <td>yeşil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kovulma</td>\n",
       "      <td>tükenme</td>\n",
       "      <td>kovulma</td>\n",
       "      <td>kovulma</td>\n",
       "      <td>mezuniyet</td>\n",
       "      <td>sakınca</td>\n",
       "      <td>yeşil</td>\n",
       "      <td>apse</td>\n",
       "      <td>dava</td>\n",
       "      <td>sade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>öfkeli</td>\n",
       "      <td>mezuniyet</td>\n",
       "      <td>kolera</td>\n",
       "      <td>pus</td>\n",
       "      <td>yürekli</td>\n",
       "      <td>sapık</td>\n",
       "      <td>şövalyelik</td>\n",
       "      <td>kasvetli</td>\n",
       "      <td>mezuniyet</td>\n",
       "      <td>kozmopolit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>günah</td>\n",
       "      <td>piyango</td>\n",
       "      <td>çürüme</td>\n",
       "      <td>kolera</td>\n",
       "      <td>maaş</td>\n",
       "      <td>anayasaya aykırı</td>\n",
       "      <td>kozmopolit</td>\n",
       "      <td>kabul edilemez</td>\n",
       "      <td>hırsız</td>\n",
       "      <td>bayım</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>antitez</td>\n",
       "      <td>Komşuluk</td>\n",
       "      <td>saygısız</td>\n",
       "      <td>forewarned</td>\n",
       "      <td>tutkulu</td>\n",
       "      <td>yama</td>\n",
       "      <td>Bilişsel</td>\n",
       "      <td>şanssızlık</td>\n",
       "      <td>kararsız</td>\n",
       "      <td>sıkılık</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dikkatsizlik</td>\n",
       "      <td>izlemek</td>\n",
       "      <td>dava</td>\n",
       "      <td>çürüme</td>\n",
       "      <td>çikolata</td>\n",
       "      <td>karartmak</td>\n",
       "      <td>ağırbaşlı</td>\n",
       "      <td>kovulma</td>\n",
       "      <td>kilitlenme</td>\n",
       "      <td>özgürlük</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saygısız</td>\n",
       "      <td>hemen</td>\n",
       "      <td>kürtaj</td>\n",
       "      <td>saygısız</td>\n",
       "      <td>canlanma</td>\n",
       "      <td>aptallık</td>\n",
       "      <td>temiz</td>\n",
       "      <td>düşmek</td>\n",
       "      <td>pop</td>\n",
       "      <td>holding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dava</td>\n",
       "      <td>sabır</td>\n",
       "      <td>porno</td>\n",
       "      <td>aşma</td>\n",
       "      <td>mübarek</td>\n",
       "      <td>maruz</td>\n",
       "      <td>telafi edici</td>\n",
       "      <td>kitabe</td>\n",
       "      <td>sarsıntı</td>\n",
       "      <td>mütevazi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>reddediyoruz</td>\n",
       "      <td>maaş</td>\n",
       "      <td>kabızlık</td>\n",
       "      <td>dava</td>\n",
       "      <td>gıdıklamak</td>\n",
       "      <td>hariç</td>\n",
       "      <td>bayım</td>\n",
       "      <td>kolera</td>\n",
       "      <td>kaybetmek</td>\n",
       "      <td>iman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>soymak</td>\n",
       "      <td>analist</td>\n",
       "      <td>oğul</td>\n",
       "      <td>eksik</td>\n",
       "      <td>aloha</td>\n",
       "      <td>körü körüne</td>\n",
       "      <td>sıkılık</td>\n",
       "      <td>çürüme</td>\n",
       "      <td>gıdıklamak</td>\n",
       "      <td>gösteri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tepki</td>\n",
       "      <td>tutkulu</td>\n",
       "      <td>reddediyoruz</td>\n",
       "      <td>kürtaj</td>\n",
       "      <td>Gökler</td>\n",
       "      <td>Alarm</td>\n",
       "      <td>atlet</td>\n",
       "      <td>saygısız</td>\n",
       "      <td>büyüklük</td>\n",
       "      <td>kooperatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>itiraz</td>\n",
       "      <td>çikolata</td>\n",
       "      <td>soymak</td>\n",
       "      <td>ölüme mahkum</td>\n",
       "      <td>özgürleşme</td>\n",
       "      <td>apse</td>\n",
       "      <td>eleştiri</td>\n",
       "      <td>dava</td>\n",
       "      <td>mucize</td>\n",
       "      <td>yuva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hırsız</td>\n",
       "      <td>kramp</td>\n",
       "      <td>sakar</td>\n",
       "      <td>soymak</td>\n",
       "      <td>Bulunan</td>\n",
       "      <td>tütsü</td>\n",
       "      <td>özgürlük</td>\n",
       "      <td>eksik</td>\n",
       "      <td>istemeden</td>\n",
       "      <td>reddediyoruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nefret</td>\n",
       "      <td>netice</td>\n",
       "      <td>hırsız</td>\n",
       "      <td>kartuş</td>\n",
       "      <td>tatil</td>\n",
       "      <td>hoşlanmama</td>\n",
       "      <td>macera</td>\n",
       "      <td>kürtaj</td>\n",
       "      <td>hava saldırısı</td>\n",
       "      <td>kayıt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sürtünme</td>\n",
       "      <td>canlanma</td>\n",
       "      <td>iki yüzlü</td>\n",
       "      <td>mezuniyet</td>\n",
       "      <td>armoni</td>\n",
       "      <td>yetki devri</td>\n",
       "      <td>mütevazi</td>\n",
       "      <td>ölüme mahkum</td>\n",
       "      <td>oynak</td>\n",
       "      <td>mezuniyet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kirpik</td>\n",
       "      <td>gıdıklamak</td>\n",
       "      <td>şekil bozukluğu</td>\n",
       "      <td>tepki</td>\n",
       "      <td>otantik</td>\n",
       "      <td>kasvetli</td>\n",
       "      <td>kooperatif</td>\n",
       "      <td>soymak</td>\n",
       "      <td>flört</td>\n",
       "      <td>yürekli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>iblis</td>\n",
       "      <td>aloha</td>\n",
       "      <td>ilmihal</td>\n",
       "      <td>hırsız</td>\n",
       "      <td>büyüklük</td>\n",
       "      <td>sade</td>\n",
       "      <td>tanıtım</td>\n",
       "      <td>tükenme</td>\n",
       "      <td>susuzluk</td>\n",
       "      <td>bütünlük</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dökümleri</td>\n",
       "      <td>özgürleşme</td>\n",
       "      <td>şarlatan</td>\n",
       "      <td>polis</td>\n",
       "      <td>torunlar</td>\n",
       "      <td>kabul edilemez</td>\n",
       "      <td>muzaffer</td>\n",
       "      <td>hırsız</td>\n",
       "      <td>peri</td>\n",
       "      <td>polis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>boşa</td>\n",
       "      <td>ulaşılabilir</td>\n",
       "      <td>çukurluğu</td>\n",
       "      <td>şekil bozukluğu</td>\n",
       "      <td>güzelleştirmek</td>\n",
       "      <td>şanssızlık</td>\n",
       "      <td>nimet</td>\n",
       "      <td>demirleme</td>\n",
       "      <td>alışveriş</td>\n",
       "      <td>kanıt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eşitsizlik</td>\n",
       "      <td>astrolog</td>\n",
       "      <td>kontamine</td>\n",
       "      <td>hakimiyet</td>\n",
       "      <td>gönüllü</td>\n",
       "      <td>kovulma</td>\n",
       "      <td>örgütlü</td>\n",
       "      <td>şekil bozukluğu</td>\n",
       "      <td>tamamlayıcısı</td>\n",
       "      <td>dayanma gücü</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ızdırap</td>\n",
       "      <td>tatil</td>\n",
       "      <td>iblis</td>\n",
       "      <td>kararsız</td>\n",
       "      <td>mucize</td>\n",
       "      <td>öfkeli</td>\n",
       "      <td>kaya</td>\n",
       "      <td>çukurluğu</td>\n",
       "      <td>uyardı</td>\n",
       "      <td>masum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ayıp</td>\n",
       "      <td>torunlar</td>\n",
       "      <td>boşa</td>\n",
       "      <td>izlemek</td>\n",
       "      <td>vefa</td>\n",
       "      <td>düşmek</td>\n",
       "      <td>mezuniyet</td>\n",
       "      <td>iblis</td>\n",
       "      <td>coşku</td>\n",
       "      <td>hakimiyet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kaybetmek</td>\n",
       "      <td>şarta</td>\n",
       "      <td>organik gübre</td>\n",
       "      <td>tugay</td>\n",
       "      <td>kalkınan</td>\n",
       "      <td>günah</td>\n",
       "      <td>yorumcu</td>\n",
       "      <td>dökümleri</td>\n",
       "      <td>sırıtış</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kapmak</td>\n",
       "      <td>gönüllü</td>\n",
       "      <td>ayıp</td>\n",
       "      <td>çukurluğu</td>\n",
       "      <td>favori</td>\n",
       "      <td>pus</td>\n",
       "      <td>yürekli</td>\n",
       "      <td>eşitsizlik</td>\n",
       "      <td>akrep</td>\n",
       "      <td>sabır</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>diken</td>\n",
       "      <td>mucize</td>\n",
       "      <td>ri</td>\n",
       "      <td>kirpik</td>\n",
       "      <td>Sevgilim</td>\n",
       "      <td>antitez</td>\n",
       "      <td>demirleme</td>\n",
       "      <td>ızdırap</td>\n",
       "      <td>çile</td>\n",
       "      <td>maaş</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hükmetmek</td>\n",
       "      <td>kalkınan</td>\n",
       "      <td>kaybetmek</td>\n",
       "      <td>iblis</td>\n",
       "      <td>oynak</td>\n",
       "      <td>kolera</td>\n",
       "      <td>bütünlük</td>\n",
       "      <td>kilitlenme</td>\n",
       "      <td>nimetler</td>\n",
       "      <td>analist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sahtekâr</td>\n",
       "      <td>köpek yavrusu</td>\n",
       "      <td>atık</td>\n",
       "      <td>dikkatlice</td>\n",
       "      <td>flört</td>\n",
       "      <td>forewarned</td>\n",
       "      <td>dayanma gücü</td>\n",
       "      <td>ağrıyan</td>\n",
       "      <td>hoş</td>\n",
       "      <td>tutkulu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaletsizlik</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bitkin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kıpırdamak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karmakarışık</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dikkat dağıtıcı</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inkâr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kamçılamak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tezat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>durgunluk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gecikme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bağnaz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>korkmuş</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kurban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kandırmak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hırsızlık</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>modası geçmiş</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ruh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yaya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ağlamak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kurtçuk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>emmek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drenaj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>acı çektirmek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kesilmiş</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>düzenbaz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Çürük</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>boykot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>imha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Güvenlik açığı</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pısırık</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1623 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0              1                2                3  \\\n",
       "0      kayıtsızlık     forewarned      kayıtsızlık      kayıtsızlık   \n",
       "1         aptallık       özgürlük            sapık            Alarm   \n",
       "2            tütsü         macera         aptallık       hoşlanmama   \n",
       "3       hoşlanmama            far       hoşlanmama       şanssızlık   \n",
       "4          kovulma        tükenme          kovulma          kovulma   \n",
       "5           öfkeli      mezuniyet           kolera              pus   \n",
       "6            günah        piyango           çürüme           kolera   \n",
       "7          antitez       Komşuluk         saygısız       forewarned   \n",
       "8     dikkatsizlik        izlemek             dava           çürüme   \n",
       "9         saygısız          hemen           kürtaj         saygısız   \n",
       "10            dava          sabır            porno             aşma   \n",
       "11    reddediyoruz           maaş         kabızlık             dava   \n",
       "12          soymak        analist             oğul            eksik   \n",
       "13           tepki        tutkulu     reddediyoruz           kürtaj   \n",
       "14          itiraz       çikolata           soymak     ölüme mahkum   \n",
       "15          hırsız          kramp            sakar           soymak   \n",
       "16          nefret         netice           hırsız           kartuş   \n",
       "17        sürtünme       canlanma        iki yüzlü        mezuniyet   \n",
       "18          kirpik     gıdıklamak  şekil bozukluğu            tepki   \n",
       "19           iblis          aloha          ilmihal           hırsız   \n",
       "20       dökümleri     özgürleşme         şarlatan            polis   \n",
       "21            boşa   ulaşılabilir        çukurluğu  şekil bozukluğu   \n",
       "22      eşitsizlik       astrolog        kontamine        hakimiyet   \n",
       "23         ızdırap          tatil            iblis         kararsız   \n",
       "24            ayıp       torunlar             boşa          izlemek   \n",
       "25       kaybetmek          şarta    organik gübre            tugay   \n",
       "26          kapmak        gönüllü             ayıp        çukurluğu   \n",
       "27           diken         mucize               ri           kirpik   \n",
       "28       hükmetmek       kalkınan        kaybetmek            iblis   \n",
       "29        sahtekâr  köpek yavrusu             atık       dikkatlice   \n",
       "...            ...            ...              ...              ...   \n",
       "1593           NaN            NaN              NaN              NaN   \n",
       "1594           NaN            NaN              NaN              NaN   \n",
       "1595           NaN            NaN              NaN              NaN   \n",
       "1596           NaN            NaN              NaN              NaN   \n",
       "1597           NaN            NaN              NaN              NaN   \n",
       "1598           NaN            NaN              NaN              NaN   \n",
       "1599           NaN            NaN              NaN              NaN   \n",
       "1600           NaN            NaN              NaN              NaN   \n",
       "1601           NaN            NaN              NaN              NaN   \n",
       "1602           NaN            NaN              NaN              NaN   \n",
       "1603           NaN            NaN              NaN              NaN   \n",
       "1604           NaN            NaN              NaN              NaN   \n",
       "1605           NaN            NaN              NaN              NaN   \n",
       "1606           NaN            NaN              NaN              NaN   \n",
       "1607           NaN            NaN              NaN              NaN   \n",
       "1608           NaN            NaN              NaN              NaN   \n",
       "1609           NaN            NaN              NaN              NaN   \n",
       "1610           NaN            NaN              NaN              NaN   \n",
       "1611           NaN            NaN              NaN              NaN   \n",
       "1612           NaN            NaN              NaN              NaN   \n",
       "1613           NaN            NaN              NaN              NaN   \n",
       "1614           NaN            NaN              NaN              NaN   \n",
       "1615           NaN            NaN              NaN              NaN   \n",
       "1616           NaN            NaN              NaN              NaN   \n",
       "1617           NaN            NaN              NaN              NaN   \n",
       "1618           NaN            NaN              NaN              NaN   \n",
       "1619           NaN            NaN              NaN              NaN   \n",
       "1620           NaN            NaN              NaN              NaN   \n",
       "1621           NaN            NaN              NaN              NaN   \n",
       "1622           NaN            NaN              NaN              NaN   \n",
       "\n",
       "                   4                 5                6                7  \\\n",
       "0            faydalı       kayıtsızlık          üretici      kayıtsızlık   \n",
       "1              yeşil            zoraki         haysiyet         aptallık   \n",
       "2           özgürlük          sahipsiz  dayanak noktası            hariç   \n",
       "3           muzaffer              meme          faydalı      körü körüne   \n",
       "4          mezuniyet           sakınca            yeşil             apse   \n",
       "5            yürekli             sapık       şövalyelik         kasvetli   \n",
       "6               maaş  anayasaya aykırı       kozmopolit   kabul edilemez   \n",
       "7            tutkulu              yama         Bilişsel       şanssızlık   \n",
       "8           çikolata         karartmak        ağırbaşlı          kovulma   \n",
       "9           canlanma          aptallık            temiz           düşmek   \n",
       "10           mübarek             maruz     telafi edici           kitabe   \n",
       "11        gıdıklamak             hariç            bayım           kolera   \n",
       "12             aloha       körü körüne          sıkılık           çürüme   \n",
       "13            Gökler             Alarm            atlet         saygısız   \n",
       "14        özgürleşme              apse         eleştiri             dava   \n",
       "15           Bulunan             tütsü         özgürlük            eksik   \n",
       "16             tatil        hoşlanmama           macera           kürtaj   \n",
       "17            armoni       yetki devri         mütevazi     ölüme mahkum   \n",
       "18           otantik          kasvetli       kooperatif           soymak   \n",
       "19          büyüklük              sade          tanıtım          tükenme   \n",
       "20          torunlar    kabul edilemez         muzaffer           hırsız   \n",
       "21    güzelleştirmek        şanssızlık            nimet        demirleme   \n",
       "22           gönüllü           kovulma          örgütlü  şekil bozukluğu   \n",
       "23            mucize            öfkeli             kaya        çukurluğu   \n",
       "24              vefa            düşmek        mezuniyet            iblis   \n",
       "25          kalkınan             günah          yorumcu        dökümleri   \n",
       "26            favori               pus          yürekli       eşitsizlik   \n",
       "27          Sevgilim           antitez        demirleme          ızdırap   \n",
       "28             oynak            kolera         bütünlük       kilitlenme   \n",
       "29             flört        forewarned     dayanma gücü          ağrıyan   \n",
       "...              ...               ...              ...              ...   \n",
       "1593             NaN      adaletsizlik              NaN              NaN   \n",
       "1594             NaN            bitkin              NaN              NaN   \n",
       "1595             NaN        kıpırdamak              NaN              NaN   \n",
       "1596             NaN      karmakarışık              NaN              NaN   \n",
       "1597             NaN   dikkat dağıtıcı              NaN              NaN   \n",
       "1598             NaN             inkâr              NaN              NaN   \n",
       "1599             NaN        kamçılamak              NaN              NaN   \n",
       "1600             NaN             tezat              NaN              NaN   \n",
       "1601             NaN         durgunluk              NaN              NaN   \n",
       "1602             NaN           gecikme              NaN              NaN   \n",
       "1603             NaN            bağnaz              NaN              NaN   \n",
       "1604             NaN           korkmuş              NaN              NaN   \n",
       "1605             NaN            kurban              NaN              NaN   \n",
       "1606             NaN         kandırmak              NaN              NaN   \n",
       "1607             NaN         hırsızlık              NaN              NaN   \n",
       "1608             NaN     modası geçmiş              NaN              NaN   \n",
       "1609             NaN               Ruh              NaN              NaN   \n",
       "1610             NaN              yaya              NaN              NaN   \n",
       "1611             NaN           ağlamak              NaN              NaN   \n",
       "1612             NaN           kurtçuk              NaN              NaN   \n",
       "1613             NaN             emmek              NaN              NaN   \n",
       "1614             NaN            drenaj              NaN              NaN   \n",
       "1615             NaN     acı çektirmek              NaN              NaN   \n",
       "1616             NaN          kesilmiş              NaN              NaN   \n",
       "1617             NaN          düzenbaz              NaN              NaN   \n",
       "1618             NaN             Çürük              NaN              NaN   \n",
       "1619             NaN            boykot              NaN              NaN   \n",
       "1620             NaN              imha              NaN              NaN   \n",
       "1621             NaN    Güvenlik açığı              NaN              NaN   \n",
       "1622             NaN           pısırık              NaN              NaN   \n",
       "\n",
       "                    8                9  \n",
       "0     ortaya çıkarmak         haysiyet  \n",
       "1              mistik  dayanak noktası  \n",
       "2               Alarm          faydalı  \n",
       "3            özgürlük            yeşil  \n",
       "4                dava             sade  \n",
       "5           mezuniyet       kozmopolit  \n",
       "6              hırsız            bayım  \n",
       "7            kararsız          sıkılık  \n",
       "8          kilitlenme         özgürlük  \n",
       "9                 pop          holding  \n",
       "10           sarsıntı         mütevazi  \n",
       "11          kaybetmek             iman  \n",
       "12         gıdıklamak          gösteri  \n",
       "13           büyüklük       kooperatif  \n",
       "14             mucize             yuva  \n",
       "15          istemeden     reddediyoruz  \n",
       "16     hava saldırısı            kayıt  \n",
       "17              oynak        mezuniyet  \n",
       "18              flört          yürekli  \n",
       "19           susuzluk         bütünlük  \n",
       "20               peri            polis  \n",
       "21          alışveriş            kanıt  \n",
       "22      tamamlayıcısı     dayanma gücü  \n",
       "23             uyardı            masum  \n",
       "24              coşku        hakimiyet  \n",
       "25            sırıtış               at  \n",
       "26              akrep            sabır  \n",
       "27               çile             maaş  \n",
       "28           nimetler          analist  \n",
       "29                hoş          tutkulu  \n",
       "...               ...              ...  \n",
       "1593              NaN              NaN  \n",
       "1594              NaN              NaN  \n",
       "1595              NaN              NaN  \n",
       "1596              NaN              NaN  \n",
       "1597              NaN              NaN  \n",
       "1598              NaN              NaN  \n",
       "1599              NaN              NaN  \n",
       "1600              NaN              NaN  \n",
       "1601              NaN              NaN  \n",
       "1602              NaN              NaN  \n",
       "1603              NaN              NaN  \n",
       "1604              NaN              NaN  \n",
       "1605              NaN              NaN  \n",
       "1606              NaN              NaN  \n",
       "1607              NaN              NaN  \n",
       "1608              NaN              NaN  \n",
       "1609              NaN              NaN  \n",
       "1610              NaN              NaN  \n",
       "1611              NaN              NaN  \n",
       "1612              NaN              NaN  \n",
       "1613              NaN              NaN  \n",
       "1614              NaN              NaN  \n",
       "1615              NaN              NaN  \n",
       "1616              NaN              NaN  \n",
       "1617              NaN              NaN  \n",
       "1618              NaN              NaN  \n",
       "1619              NaN              NaN  \n",
       "1620              NaN              NaN  \n",
       "1621              NaN              NaN  \n",
       "1622              NaN              NaN  \n",
       "\n",
       "[1623 rows x 10 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_df.to_csv('NRCLexicon/turkish_nrc.csv',index=False)\n",
    "lexicon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon_df = pd.read_csv('NRCLexicon/turkish_nrc.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec\n"
     ]
    }
   ],
   "source": [
    "###### STEP 3: Loading Word2Vec Model:\n",
    "#print (\"Loading Word2Vec\")\n",
    "#from gensim.models import word2vec\n",
    "#model = word2vec.Word2Vec.load('../../Project_Backup/BigData/Models/whole_it_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'insulto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-facc477c30c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'insulto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pirata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\fnac\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fnac\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\fnac\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'insulto'"
     ]
    }
   ],
   "source": [
    "model.similarity('insulto','pirata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_matrix_sentences_list_word2vec(nava_words, nrc_lexicon,model):\n",
    "    \"\"\"\n",
    "\n",
    "    :param word2vec model:\n",
    "    :param nava_words: we can pass any version of the bag of words\n",
    "    :param nrc_lexicon:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sm_list = list_nrc_lexicon(nrc_lexicon)\n",
    "    emotions = nrc_lexicon.columns.values\n",
    "    matrix_sentences_list = []\n",
    "    for i in tqdm(range(0, len(nava_words))): # Iterate over all sentences\n",
    "        \" Initialize matrix for each sentence \"\n",
    "        w, h = len(nava_words[i]), 10\n",
    "        matrix_sentence = [[0 for x in range(w)] for y in range(h)]\n",
    "        k = 0\n",
    "        for word in nava_words[i]: # Iterate over all words in the sentence\n",
    "            j = 0\n",
    "            for emotion in range(0, len(emotions)): # Iterate over all emotions => fill in the emotional vectors for all words\n",
    "                total_similarity = 0\n",
    "                for representative_word in sm_list[emotion][0:10]:\n",
    "                    r = len(sm_list[emotion])\n",
    "                    if word in model and representative_word in model:\n",
    "                        total_similarity += model.similarity(word, representative_word)\n",
    "                matrix_sentence[j][k] += total_similarity / r \n",
    "                j += 1 # increment index of representative words\n",
    "            k += 1 # increment index of transcript words\n",
    "        # append the matrix_sentence to the global list for all sentences\n",
    "        matrix_sentences_list.append(matrix_sentence)\n",
    "    return matrix_sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hb']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word level scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 331035/331035 [44:36<00:00, 123.66it/s]\n"
     ]
    }
   ],
   "source": [
    "###### STEP 4: Word Level\n",
    "print (\"Computing word level scores\")\n",
    "matrix_sentences_word2vec = compute_matrix_sentences_list_word2vec(nava_tweets,lexicon_df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0057116875423981039, 0.0078047429297374085],\n",
       " [0.0060167928180368145, 0.01023575849751735],\n",
       " [0.0061844762299235389, 0.0093637660441997748],\n",
       " [0.0029643438294465266, 0.0043628413126492243],\n",
       " [0.010222436134512642, 0.016063174154950687],\n",
       " [0.0021053958348313797, 0.0035463607810704862],\n",
       " [0.0020871663320264386, 0.003241831617149262],\n",
       " [0.0039944016485211065, 0.0063264527756776373],\n",
       " [0.013639043814702283, 0.020052841149304629],\n",
       " [0.0058338785292868529, 0.0087626098500389483]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_sentences_word2vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hb']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def compute_sentence_emotion_vectors(matrix_sentences_list):\n",
    "    emotion_vector_list = []\n",
    "    for i in tqdm(range(0, len(matrix_sentences_list))):\n",
    "        sum_sentence = []\n",
    "        ids = [0,1,2,3,4,7,8,9]\n",
    "        for j in ids: # for each emotion\n",
    "            sum_words = 0\n",
    "            for k in range(0, len(matrix_sentences_list[i][j])):\n",
    "                sum_words += matrix_sentences_list[i][j][k]*1000\n",
    "            r = len(matrix_sentences_list[i])\n",
    "            if r != 0 :\n",
    "                sum_words = sum_words / r # Arithmetic mean\n",
    "            sum_sentence.append(sum_words)\n",
    "        emotion_vector_list.append(sum_sentence)\n",
    "    return emotion_vector_list\n",
    "\n",
    "def compute_sentence_sentiment_vectors(matrix_sentences_list):\n",
    "    emotion_vector_list = []\n",
    "    for i in tqdm(range(0, len(matrix_sentences_list))):\n",
    "        sum_sentence = []\n",
    "        ids = [5,6]\n",
    "        for j in ids: # for each emotion\n",
    "            sum_words = 0\n",
    "            for k in range(0, len(matrix_sentences_list[i][j])):\n",
    "                sum_words += matrix_sentences_list[i][j][k]*1000\n",
    "            r = len(matrix_sentences_list[i])\n",
    "            if r != 0 : \n",
    "                sum_words = sum_words / r\n",
    "            sum_sentence.append(sum_words)\n",
    "        emotion_vector_list.append(sum_sentence)\n",
    "    return emotion_vector_list\n",
    "\n",
    "def compute_emotionalities(sentence_vectors):\n",
    "    emotionalities = []\n",
    "    threshold = 0 # THRESHOLD PARAMETER TO BE FINE TUNED (0 for lexicon, 0.2 for pmi)\n",
    "    for i in tqdm(range(0,len(sentence_vectors))):\n",
    "        sentence_vector = sentence_vectors[i]\n",
    "        mylist = [0 if math.isnan(x) else x for x in sentence_vector]\n",
    "        if (max(mylist) > threshold): #Threshold \n",
    "            emotionalities.append(sentence_vectors[i].index(max(mylist)))\n",
    "        else: \n",
    "            emotionalities.append(8)\n",
    "    return emotionalities\n",
    "\n",
    "def compute_sentiments(sentence_vectors_sent,emotionalities):\n",
    "    sentiments = []\n",
    "    threshold = 0 # THRESHOLD PARAMETER TO BE FINE TUNED (0 for lexicon, 0.2 for pmi)\n",
    "    for i in tqdm(range(0,len(sentence_vectors_sent))):\n",
    "        sentence_vector = sentence_vectors_sent[i]\n",
    "        mylist = [0 if math.isnan(x) else x for x in sentence_vector]\n",
    "        if (max(mylist) > threshold): #Threshold \n",
    "            sentiments.append(sentence_vectors_sent[i].index(max(mylist)))\n",
    "        else:\n",
    "            # To increase Recall, we also use emotionalities, in case a tweet is neutral\n",
    "            if emotionalities[i] in [0,2,3,5]:\n",
    "                sentiments.append(0) # Negative Emotion\n",
    "            if emotionalities[i] in [1,4,6,7]:\n",
    "                sentiments.append(1) # Positive Emotion\n",
    "            if emotionalities[i] == 8:\n",
    "                sentiments.append(2) # Otherwise, we just return Neutral\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Emotionalities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 331035/331035 [00:13<00:00, 24900.24it/s]\n",
      "100%|██████████████████████████████| 331035/331035 [00:01<00:00, 293812.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentiments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 331035/331035 [00:03<00:00, 101289.75it/s]\n",
      "100%|██████████████████████████████| 331035/331035 [00:00<00:00, 484557.23it/s]\n"
     ]
    }
   ],
   "source": [
    "###### STEP 5: Sentence Level:\n",
    "print (\"Computing Emotionalities\")\n",
    "# Emotion Recognition\n",
    "sentence_vectors_word2vec = compute_sentence_emotion_vectors(matrix_sentences_word2vec)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_word2vec)\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "print (\"Computing sentiments\")\n",
    "sentence_vectors_sent = compute_sentence_sentiment_vectors(matrix_sentences_word2vec)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent,emotionalities)\n",
    "\n",
    "###### FINAL STEP 6: Storing Emotion + Sentiment for each tweet\n",
    "\n",
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    5: 'Sadness',\n",
    "    6: 'Surprise',\n",
    "    7: 'Trust',\n",
    "    8: 'Neutral'\n",
    "}\n",
    "sent_dict = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Positive\",\n",
    "    2: \"Neutral\"\n",
    "}\n",
    "\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4284738775261361,\n",
       " 1.9759870158033888,\n",
       " 1.7182788249045373,\n",
       " 0.74582752934429064,\n",
       " 3.1997340641994776,\n",
       " 1.08907041512743,\n",
       " 3.839270773118701,\n",
       " 1.6812337177804835]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors_word2vec[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in dataframe\n"
     ]
    }
   ],
   "source": [
    "print (\"Storing in dataframe\")\n",
    "word2vec_results_df = pd.DataFrame()\n",
    "\n",
    "word2vec_results_df['Nava Tweet'] = nava_tweets\n",
    "\n",
    "word2vec_results_df['Emotion Vectors'] = sentence_vectors_word2vec\n",
    "\n",
    "word2vec_results_df['Emotion'] = emotions\n",
    "\n",
    "word2vec_results_df['Sentiment Vectors'] = sentence_vectors_sent\n",
    "\n",
    "word2vec_results_df['Sentiment'] = senti\n",
    "\n",
    "word2vec_results_df.to_csv('../../Project_Backup/BigData/Word2VecBasedResults/tr/Tweets_Labelled_Word2Vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nava Tweet</th>\n",
       "      <th>Emotion Vectors</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment Vectors</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[iphoon, meten, nexus]</td>\n",
       "      <td>[0.884920438723, 2.30446919141, 1.0439055915, ...</td>\n",
       "      <td>Surprise</td>\n",
       "      <td>[0.260638537939, 0.354363314745]</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[beginning, feel, home, eten, hangkant]</td>\n",
       "      <td>[1.23918403443, 2.73380914632, 1.25902353457, ...</td>\n",
       "      <td>Surprise</td>\n",
       "      <td>[0.310734460599, 0.3274642242]</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ueeeeeeeeeeeeeooo]</td>\n",
       "      <td>[0.0674638401601, 0.135911464138, 0.0636923252...</td>\n",
       "      <td>Surprise</td>\n",
       "      <td>[0.0186801057148, 0.0133107461327]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[arzurich, developer, designer, geotaggers, al...</td>\n",
       "      <td>[3.67700226412, 8.89141266243, 4.24250307971, ...</td>\n",
       "      <td>Surprise</td>\n",
       "      <td>[1.0491679129, 1.30976516478]</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tapas, et, hoegaard, joli, apéro, chez, mathi...</td>\n",
       "      <td>[1.14908027863, 3.52168975666, 1.09623750354, ...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>[0.42267562281, 0.566264562012]</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Nava Tweet  \\\n",
       "0                             [iphoon, meten, nexus]   \n",
       "1            [beginning, feel, home, eten, hangkant]   \n",
       "2                                [ueeeeeeeeeeeeeooo]   \n",
       "3  [arzurich, developer, designer, geotaggers, al...   \n",
       "4  [tapas, et, hoegaard, joli, apéro, chez, mathi...   \n",
       "\n",
       "                                     Emotion Vectors   Emotion  \\\n",
       "0  [0.884920438723, 2.30446919141, 1.0439055915, ...  Surprise   \n",
       "1  [1.23918403443, 2.73380914632, 1.25902353457, ...  Surprise   \n",
       "2  [0.0674638401601, 0.135911464138, 0.0636923252...  Surprise   \n",
       "3  [3.67700226412, 8.89141266243, 4.24250307971, ...  Surprise   \n",
       "4  [1.14908027863, 3.52168975666, 1.09623750354, ...       Joy   \n",
       "\n",
       "                    Sentiment Vectors Sentiment  \n",
       "0    [0.260638537939, 0.354363314745]  Negative  \n",
       "1      [0.310734460599, 0.3274642242]  Negative  \n",
       "2  [0.0186801057148, 0.0133107461327]  Positive  \n",
       "3       [1.0491679129, 1.30976516478]  Negative  \n",
       "4     [0.42267562281, 0.566264562012]  Negative  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_results_df.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
