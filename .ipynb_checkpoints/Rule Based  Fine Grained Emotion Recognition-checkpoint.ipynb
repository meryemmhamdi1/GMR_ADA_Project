{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meryem/miniconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, \"/media/diskD/EPFL/Fall 2016/ADA/Project/GMR_ADA_Project/EmotionAnalysis\")\n",
    "from DataSchemaExtractionParsing import *\n",
    "from DataPreProcessing import *\n",
    "from SentSemanticModule import *\n",
    "from SentTweetModule import *\n",
    "from SentSyntacticModule import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading English Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meryem/miniconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Actor</th>\n",
       "      <th>MovieId</th>\n",
       "      <th>SeriesName</th>\n",
       "      <th>Subtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBT</td>\n",
       "      <td>So if a photon is directed through a plane wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBT</td>\n",
       "      <td>Agreed, what is your point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBT</td>\n",
       "      <td>There’s no point, I just think it is a good i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBT</td>\n",
       "      <td>Excuse me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BBT</td>\n",
       "      <td>Hang on.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1         Actor MovieId SeriesName  \\\n",
       "0           0             0       Sheldon     NaN        BBT   \n",
       "1           1             1       Leonard     NaN        BBT   \n",
       "2           2             2       Sheldon     NaN        BBT   \n",
       "3           3             3       Leonard     NaN        BBT   \n",
       "4           4             4  Receptionist     NaN        BBT   \n",
       "\n",
       "                                            Subtitle  \n",
       "0   So if a photon is directed through a plane wi...  \n",
       "1                        Agreed, what is your point?  \n",
       "2   There’s no point, I just think it is a good i...  \n",
       "3                                         Excuse me?  \n",
       "4                                           Hang on.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets = pd.read_csv(\"/home/meryem/Dropbox/meryem/algorithms/EmotionRecognition/Notebooks/WholeTranscriptDatasetWithoutContractions.txt\",encoding =\"utf-8\")\n",
    "english_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'userId', u'createdAt', u'text', u'longitude', u'latitude',\n",
       "       u'placeId', u'inReplyTo', u'source', u'truncated', u'placeLatitude',\n",
       "       u'placeLongitude', u'sourceName', u'sourceUrl', u'userName',\n",
       "       u'screenName', u'followersCount', u'friendsCount', u'statusesCount',\n",
       "       u'userLocation', u'swiss', u'canton', u'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['text'] = english_tweets['Subtitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' 120?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets['Subtitle'].iloc[877]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Special Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/diskD/EPFL/Fall 2016/ADA/Project/GMR_ADA_Project/EmotionAnalysis/DataPreProcessing.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  sample_tweets['text'] = tweets_list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_categories = handle_special_categories(english_tweets[0:1000])\n",
    "len(replaced_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it is unobserved it will, however, if it is observed after it is left the plane but before it hits its target, it will not have gone through both slits.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_categories['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing contractions (needed for more accurate tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_no_contractions = replace_contractions(replaced_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u' So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it is unobserved it will, however, if it is observed after it is left the plane but before it hits its target, it will not have gone through both slits.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_contractions['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tokenization of Tweets into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_list = bag_of_word_representation(tweets_no_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'So',\n",
       " u'if',\n",
       " u'a',\n",
       " u'photon',\n",
       " u'is',\n",
       " u'directed',\n",
       " u'through',\n",
       " u'a',\n",
       " u'plane',\n",
       " u'with',\n",
       " u'two',\n",
       " u'slits',\n",
       " u'in',\n",
       " u'it',\n",
       " u'and',\n",
       " u'either',\n",
       " u'slit',\n",
       " u'is',\n",
       " u'observed',\n",
       " u'it',\n",
       " u'will',\n",
       " u'not',\n",
       " u'go',\n",
       " u'through',\n",
       " u'both',\n",
       " u'slits',\n",
       " u'If',\n",
       " u'it',\n",
       " u'is',\n",
       " u'unobserved',\n",
       " u'it',\n",
       " u'will',\n",
       " u'however',\n",
       " u'if',\n",
       " u'it',\n",
       " u'is',\n",
       " u'observed',\n",
       " u'after',\n",
       " u'it',\n",
       " u'is',\n",
       " u'left',\n",
       " u'the',\n",
       " u'plane',\n",
       " u'but',\n",
       " u'before',\n",
       " u'it',\n",
       " u'hits',\n",
       " u'its',\n",
       " u'target',\n",
       " u'it',\n",
       " u'will',\n",
       " u'not',\n",
       " u'have',\n",
       " u'gone',\n",
       " u'through',\n",
       " u'both',\n",
       " u'slits']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'So', 'RB'),\n",
       " (u'if', 'IN'),\n",
       " (u'a', 'DT'),\n",
       " (u'photon', 'NN'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'directed', 'VBN'),\n",
       " (u'through', 'IN'),\n",
       " (u'a', 'DT'),\n",
       " (u'plane', 'NN'),\n",
       " (u'with', 'IN'),\n",
       " (u'two', 'CD'),\n",
       " (u'slits', 'NNS'),\n",
       " (u'in', 'IN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'and', 'CC'),\n",
       " (u'either', 'DT'),\n",
       " (u'slit', 'NN'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'observed', 'VBN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'will', 'MD'),\n",
       " (u'not', 'RB'),\n",
       " (u'go', 'VB'),\n",
       " (u'through', 'IN'),\n",
       " (u'both', 'DT'),\n",
       " (u'slits', 'NNS'),\n",
       " (u'If', 'IN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'unobserved', 'JJ'),\n",
       " (u'it', 'PRP'),\n",
       " (u'will', 'MD'),\n",
       " (u'however', 'RB'),\n",
       " (u'if', 'IN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'observed', 'VBN'),\n",
       " (u'after', 'IN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'left', 'VBN'),\n",
       " (u'the', 'DT'),\n",
       " (u'plane', 'NN'),\n",
       " (u'but', 'CC'),\n",
       " (u'before', 'IN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'hits', 'VBZ'),\n",
       " (u'its', 'PRP$'),\n",
       " (u'target', 'NN'),\n",
       " (u'it', 'PRP'),\n",
       " (u'will', 'MD'),\n",
       " (u'not', 'RB'),\n",
       " (u'have', 'VB'),\n",
       " (u'gone', 'VBN'),\n",
       " (u'through', 'IN'),\n",
       " (u'both', 'DT'),\n",
       " (u'slits', 'NNS')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tweets = pos_tagging(tokenized_list)\n",
    "tagged_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unannotated_tweets_df = pd.read_csv('Results/nava representation.csv')\n",
    "unannotated_tweets_df['Tokenized Lemmatized'] = lemmatizer_raw(tagged_tweets)\n",
    "unannotated_tweets_df.to_csv('Results/Unannotated_Representation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STANFORD VERSION : More accurate but is too slow:\n",
    "#import os\n",
    "#os.environ[\"STANFORD_MODELS\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "#os.envir\n",
    "#on[\"STANFORD_PARSER\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "#from nltk.parse.stanford import StanfordDependencyParser\n",
    "#dep_parser=StanfordDependencyParser(model_path=\"/home/meryem/Downloads/stanford-parser-full-2016-10-31/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "#dependency_trees = []\n",
    "#for tweet in tweets_no_contractions['text']:\n",
    "    #result = dep_parser.raw_parse(\"No rest is detrimental\")\n",
    "    #dep = result.next()\n",
    "    #dependency_trees.append(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION FOR DEPENDENCY PARSING:\n",
    "# to install Run the following:\n",
    "# pip install -U spacy\n",
    "# python -m spacy.en.download all # for ENGLISH\n",
    "# python -m spacy.de.download all # for Deutch\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "# Joining text:\n",
    "tweets_text = []\n",
    "for i in range(0, len(tokenized_list)):\n",
    "    space = u\" \"\n",
    "    tweets_text.append(space.join(tokenized_list[i]))\n",
    "tweets_text[0].encode(\"utf-8\")\n",
    "for i in range(0, len(tweets_text)):\n",
    "    doc = nlp(tweets_text[i])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u'')\n",
    "\n",
    "#tweets_text[877]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples = []\n",
    "for sample in docs:\n",
    "    new_samples_sub = []\n",
    "    for word in sample:\n",
    "        new_samples_sub.append((unicode(word),word.pos_))\n",
    "    new_samples.append(new_samples_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Syntactic Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples,triple_dependencies = apply_syntactic_rules(docs,new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<< Original tweet text >>>\n",
      "\n",
      "So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits If it is unobserved it will however if it is observed after it is left the plane but before it hits its target it will not have gone through both slits\n",
      "\n",
      "<<<< Syntactic dependencies >>>\n",
      "\n",
      "[(So, u'advmod', observed), (if, u'mark', directed), (a, u'det', photon), (photon, u'nsubjpass', directed), (is, u'auxpass', directed), (directed, u'advcl', observed), (through, u'prep', directed), (a, u'det', plane), (plane, u'pobj', through), (with, u'prep', directed), (two, u'nummod', slits), (slits, u'pobj', with), (in, u'prep', slits), (it, u'pobj', in), (and, u'cc', directed), (either, u'advmod', slit), (slit, u'conj', directed), (is, u'auxpass', observed), (observed, u'ROOT', observed), (it, u'nsubj', go), (will, u'aux', go), (not, u'neg', go), (go, u'ccomp', observed), (through, u'prep', go), (both, u'det', slits), (slits, u'pobj', through), (If, u'mark', is), (it, u'nsubj', is), (is, u'advcl', observed), (unobserved, u'acomp', is), (it, u'nsubjpass', observed), (will, u'aux', observed), (however, u'advmod', observed), (if, u'mark', observed), (it, u'nsubjpass', observed), (is, u'auxpass', observed), (observed, u'ROOT', observed), (after, u'mark', left), (it, u'nsubjpass', left), (is, u'auxpass', left), (left, u'advcl', observed), (the, u'det', plane), (plane, u'dobj', left), (but, u'cc', left), (before, u'mark', hits), (it, u'nsubj', hits), (hits, u'advcl', gone), (its, u'poss', target), (target, u'dobj', hits), (it, u'nsubj', gone), (will, u'aux', gone), (not, u'neg', gone), (have, u'aux', gone), (gone, u'ccomp', observed), (through, u'prep', gone), (both, u'det', slits), (slits, u'pobj', through)]\n",
      "\n",
      "<<<< Tweet after applying syntactic Rules >>>\n",
      "\n",
      "[u'if', u'a', u'photon', u'directed', u'through', u'a', u'plane', u'with', u'two', u'slits', u'in', u'it', u'and', u'slit', u'observed', u'it', u'will', u'through', u'both', u'slits', u'If', u'it', u'unobserved', u'it', u'will', u'if', u'it', u'observed', u'after', u'it', u'left', u'the', u'plane', u'but', u'before', u'it', u'hits', u'its', u'target', u'it', u'will', u'have', u'through', u'both', u'slits', 'not_go', 'not_gone']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print \"\\n<<<< Original tweet text >>>\\n\"\n",
    "print tweets_text[i]\n",
    "print \"\\n<<<< Syntactic dependencies >>>\\n\"\n",
    "print triple_dependencies[i]\n",
    "print \"\\n<<<< Tweet after applying syntactic Rules >>>\\n\"\n",
    "new_tweet = []\n",
    "for (word,pos) in new_samples[i]:\n",
    "    new_tweet.append(word)\n",
    "print new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Still', 'RB'), (u'the', 'DT'), (u'best', 'JJS'), (u'coffee', 'NN'), (u'in', 'IN'), (u'town', 'NN'), (u'at', 'IN'), (u'La', 'NNP'), (u'Stanza', 'NNP')]\n",
      "\n",
      "\n",
      "[(u'the', u'DET'), (u'best', u'ADJ'), (u'in', u'ADP'), (u'town', u'NOUN'), (u'at', u'ADP'), (u'La', u'PROPN'), (u'Stanza', u'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_tweets[0]\n",
    "print \"\\n\"\n",
    "print new_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_without_ne = remove_named_entities(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ready', u'ADJ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tags = normalize_pos_tags_words(tweet_without_ne)\n",
    "normalized_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation and Stop words and Converting to Lower Case and Removal of Other special categories: url, number, username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_tweets_without = eliminate_stop_words_punct(normalized_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'best', u'ADJ'), (u'town', 'n'), (u'la', u'PROPN'), (u'stanza', u'PROPN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tweets = lemmatizer(tagged_tweets_without)\n",
    "\n",
    "lemmatized_tweets_untag = lemmatizer_untagged(tagged_tweets_without)\n",
    "lemmatized_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only NAVA words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nava_tweets = keep_only_nava_words(lemmatized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best', u'town']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nava_tweets_df = pd.DataFrame()\n",
    "nava_tweets_df['Nava Tweets'] = nava_tweets\n",
    "nava_tweets_df.to_csv('Results/nava representation.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting NRC Lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE THIS TO DIRECTLY LOAD THE PROCESSED LEXICON\n",
    "lexicon_df = pd.read_csv('NRCLexicon/lexicon_nrc.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon = extractLexicon()\n",
    "word_set = list(set(lexicon['Word']))\n",
    "word_emotional_vectors = []\n",
    "for word in word_set:\n",
    "    word_emotional_vectors.append((word,list(lexicon[lexicon['Word']==word]['Score'])))\n",
    "word_emotional_vectors_dict = dict(word_emotional_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '0', '0', '1', '0', '1', '0', '0', '1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.head(1)\n",
    "word_emotional_vectors_dict['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    5: 'Negative',\n",
    "    6: 'Positive',\n",
    "    7: 'Sadness',\n",
    "    8: 'Surprise',\n",
    "    9: 'Trust',\n",
    "    10: 'Neutral'\n",
    "}\n",
    "emotion_ids = []\n",
    "for word in word_emotional_vectors_dict.keys():\n",
    "    for i in range(0,len(word_emotional_vectors_dict[word])):\n",
    "        if word_emotional_vectors_dict[word][i] == '1':\n",
    "            emotion_ids.append((word,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3324"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_repres_words_list = [] \n",
    "for i in range(0,10):\n",
    "    emotion_repres_words_list_sub = []\n",
    "    for (word,emotion) in emotion_ids:\n",
    "        if emotion == i:\n",
    "            emotion_repres_words_list_sub.append(word)\n",
    "    emotion_repres_words_list.append(emotion_repres_words_list_sub)\n",
    "len(emotion_repres_words_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon_df = pd.DataFrame()\n",
    "lexicon_df[0] = emotion_repres_words_list[0]\n",
    "for i in range(1,10):\n",
    "    df = pd.DataFrame()\n",
    "    df[i] = emotion_repres_words_list[i]\n",
    "    lexicon_df= pd.concat([lexicon_df,df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon_df.to_csv('NRCLexicon/lexicon_nrc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lexicon_df = pd.read_csv('NRCLexicon/lexicon_nrc.csv')\n",
    "lexicon_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_lexicon = make_unique_lexicon(lexicon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Semantic Similarity using PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7ba784b9a36d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflatten_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmatized_tweets\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "flatten_list = [word for sublist in lemmatized_tweets for (word, tag) in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_pmi_dict = calculate_pmi(flatten_list,unique_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_pmi_based = compute_matrix_sentences_list(nava_tweets,lexicon_df, clean_pmi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    #5: 'Negative',\n",
    "    #6: 'Positive',\n",
    "    5: 'Sadness',\n",
    "    6: 'Surprise',\n",
    "    7: 'Trust',\n",
    "    8: 'Neutral'\n",
    "}\n",
    "sent_dict = {\n",
    "    0: \"Positive\",\n",
    "    1: \"Negative\",\n",
    "    2: \"Neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emotion Recognition\n",
    "sentence_vectors_pmi = compute_sentence_emotion_vectors(emotion_pmi_based)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_pmi)\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentence_vectors_sent_pmi = compute_sentence_sentiment_vectors(emotion_pmi_based)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['Affective Feature Representation'] = lemmatized_tweets\n",
    "english_tweets['Emotion Ids'] = sentence_vectors_pmi\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])\n",
    "english_tweets['Emotionalities'] = emotions\n",
    "english_tweets['Sentiments'] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets_col = english_tweets[['text','Affective Feature Representation','Emotion Ids','Emotionalities','Sentiments']]\n",
    "english_tweets_col.to_csv('PMI_Lexicon_ResultsSampleEnglishData.csv',encoding =\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Emotionalities'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(9)\n",
    "cs=cm.Set1(np.arange(9)/9.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Fine-grained Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Sentiments'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(3)\n",
    "cs=cm.Set1(np.arange(3)/3.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Sentiment Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Same approach using word2vec similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training Word2Vec Model on the whole tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_tokenized_lemma = lemmatizer_raw(tagged_tweets)\n",
    "len(raw_tokenized_lemma)\n",
    "raw_tokenized_lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_word2Vec_model(300, 40, 4, 10, 1e-3 , raw_tokenized_lemma, \"geo_tweets_word2vec_model\"): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.similarity('happy','so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_word2vec_based = compute_matrix_sentences_list_word2vec(lemmatized_tweets,lexicon_df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotion_word2vec_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Emotion Recognition\n",
    "sentence_vectors_word2vec = compute_sentence_emotion_vectors(emotion_word2vec_based)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_word2vec)\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentence_vectors_sent_word2vec = compute_sentence_sentiment_vectors(emotion_word2vec_based)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['Affective Feature Representation'] = lemmatized_tweets\n",
    "english_tweets['Emotion Ids'] = sentence_vectors_pmi\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])\n",
    "english_tweets['Emotionalities'] = emotions\n",
    "english_tweets['Sentiments'] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_tweets_col = english_tweets[['text','Affective Feature Representation','Emotion Ids','Emotionalities','Sentiments']]\n",
    "english_tweets_col.to_csv('Word2Vec_Lexicon_ResultsSampleEnglishData.csv',encoding =\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Emotionalities'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(9)\n",
    "cs=cm.Set1(np.arange(9)/9.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Fine-grained Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Sentiments'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(3)\n",
    "cs=cm.Set1(np.arange(3)/3.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Sentiment Emotion Distribution\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
