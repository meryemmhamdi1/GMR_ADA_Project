{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Approach for Emotion Analysis\n",
    "\n",
    "In this notebook, we apply a simple approach to capture the emotions expressed in tweets.\n",
    "first, we clean the tweet by removing https links,hashtags, and mentions. then we remove stop words.\n",
    "after that we use the NRC Emotion lexicon to look for emotion mapping for each word in a tweet and generate an emotion vector with 8 entries for each tweet, each entry in the vector represent a single emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import  csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## text cleaning\n",
    "\n",
    "def clean_up_text(text):\n",
    "    '''\n",
    "    remove https links, hashtags and mentions\n",
    "    '''\n",
    "    hashtag = re.compile(r'[#]\\w*')\n",
    "    https = re.compile(r'https?:\\/\\/[a-zA-z0-9\\/#%\\.]+')\n",
    "    mention = re.compile(r'[@]\\w*')\n",
    "    text = re.sub(hashtag, '', text)\n",
    "    text = re.sub(https, '', text)\n",
    "    text = re.sub(mention, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text_list,stop_word_language):\n",
    "    print(\"remove stop words start\")\n",
    "    c_t=[]\n",
    "    for text in text_list:\n",
    "        t=clean_up_text(text)\n",
    "        c_t.append(t)\n",
    "        \n",
    "    nostopwords_lower_list=[]\n",
    "    i=0\n",
    "    for text in c_t:\n",
    "        words_list=nltk.tokenize.word_tokenize(text)\n",
    "        filtered_words = [word for word in words_list if word not in stopwords.words(stop_word_language)]\n",
    "        filtered_words=' '.join(filtered_words)\n",
    "        nostopwords_lower_list.append(filtered_words.lower())\n",
    "        if (i%10000 == 0):\n",
    "            print(i)\n",
    "        i+=1\n",
    "    return nostopwords_lower_list\n",
    "    #do more text cleaning \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emotion_analysis(tweet_file_path, language, stop_word_language):\n",
    "    tweet=pd.read_csv(tweet_file_path)\n",
    "    print(\"number of weets: \",len(tweet))\n",
    "    #text_list=tweet['text'].tolist()\n",
    "    #nostopwords_lower_list=remove_stop_words(text_list,stop_word_language)\n",
    "    #tweet['cleaned_text']=nostopwords_lower_list\n",
    "    \n",
    "    \n",
    "    nostopwords_lower_list=tweet['cleaned_text'].tolist()\n",
    "    \n",
    "    \n",
    "    lec=pd.read_excel('NRC-Emotion-Lexicon-v0.92-InManyLanguages-web.xlsx')\n",
    "    lec_language=lec[[language,'Positive','Negative','Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust']]\n",
    "    lec_language[language]=lec_language[language].str.lower()\n",
    "    lec_language=lec_language.drop_duplicates(language)\n",
    "    lec_language=lec_language.set_index([language])\n",
    "\n",
    "    print(\"start emotion analysis\")\n",
    "    ee=0\n",
    "    emotion_final_list=[]\n",
    "    #y=1\n",
    "    for text in nostopwords_lower_list:\n",
    "        s=str(text).split(' ')\n",
    "        #tweet=pd.DataFrame()\n",
    "        emotion_list=[]\n",
    "        for ss in s:\n",
    "            if ss in lec_language.index:\n",
    "\n",
    "            #row=lec_language.loc[lec_language[language] == ss]\n",
    "                row=lec_language.loc[ss]\n",
    "\n",
    "\n",
    "                ll=[row[0],row[1],row[2],row[3],\n",
    "                    row[4],row[5],\n",
    "                    row[6],row[7],row[8],row[9]]\n",
    "                \n",
    "                emotion_list.append(ll)\n",
    "\n",
    "        #print(\"tweet: \", y)\n",
    "        #y+=1\n",
    "        #print(emotion_list)\n",
    "\n",
    "        k=[sum(i) for i in zip(*emotion_list)]\n",
    "        if(k==[]):\n",
    "            emotion_final_list.append([0,0,0,0,0,0,0,0,0,0])\n",
    "        else:\n",
    "            emotion_final_list.append(k)\n",
    "        ee+=1\n",
    "        if (ee%100000 == 0):\n",
    "            print(ee)\n",
    "    \n",
    "    tweet=pd.read_csv(tweet_file_path)\n",
    "    tweet['cleaned_text']=nostopwords_lower_list\n",
    "    \n",
    "    headers=['Positive','Negative','Anger','Anticipation','Disgust','Fear','Joy','Sadness','Surprise','Trust']\n",
    "    df = pd.DataFrame(emotion_final_list, columns=headers)\n",
    "    \n",
    "    \n",
    "    tweet=pd.concat([tweet,df], axis=1)\n",
    "    \n",
    "    return tweet\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/en_cleaned.csv', 'English Word','english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweets_with_emptions.to_csv('results/en_emotions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/fr_cleaned.csv', 'French Translation (Google Translate)','french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>canton</th>\n",
       "      <th>language</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.517199e+09</td>\n",
       "      <td>14393717.0</td>\n",
       "      <td>2010-02-23 08:02:57</td>\n",
       "      <td>Un peu de réconfort liquide en take away après...</td>\n",
       "      <td>VD</td>\n",
       "      <td>fr</td>\n",
       "      <td>un peu réconfort liquide take away après début...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.518015e+09</td>\n",
       "      <td>14393717.0</td>\n",
       "      <td>2010-02-23 08:40:13</td>\n",
       "      <td>Au charbon! (@ BCV St-François) http://4sq.com...</td>\n",
       "      <td>VD</td>\n",
       "      <td>fr</td>\n",
       "      <td>au charbon ! ( bcv st-françois )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.525122e+09</td>\n",
       "      <td>14465180.0</td>\n",
       "      <td>2010-02-23 13:20:45</td>\n",
       "      <td>C'est quoi un laptop geek? Un lapsus! :)</td>\n",
       "      <td>VD</td>\n",
       "      <td>fr</td>\n",
       "      <td>c'est quoi laptop geek ? un lapsus ! : )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.567369e+09</td>\n",
       "      <td>6589882.0</td>\n",
       "      <td>2010-02-24 08:10:43</td>\n",
       "      <td>Dans le train pour Genève</td>\n",
       "      <td>VD</td>\n",
       "      <td>fr</td>\n",
       "      <td>dans train genève</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.571348e+09</td>\n",
       "      <td>14393717.0</td>\n",
       "      <td>2010-02-24 11:06:29</td>\n",
       "      <td>Argh, pas de phó! (@ Goûts d'Asie) http://4sq....</td>\n",
       "      <td>VD</td>\n",
       "      <td>fr</td>\n",
       "      <td>argh , phó ! ( goûts d'asie )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      userId            createdAt  \\\n",
       "0  9.517199e+09  14393717.0  2010-02-23 08:02:57   \n",
       "1  9.518015e+09  14393717.0  2010-02-23 08:40:13   \n",
       "2  9.525122e+09  14465180.0  2010-02-23 13:20:45   \n",
       "3  9.567369e+09   6589882.0  2010-02-24 08:10:43   \n",
       "4  9.571348e+09  14393717.0  2010-02-24 11:06:29   \n",
       "\n",
       "                                                text canton language  \\\n",
       "0  Un peu de réconfort liquide en take away après...     VD       fr   \n",
       "1  Au charbon! (@ BCV St-François) http://4sq.com...     VD       fr   \n",
       "2           C'est quoi un laptop geek? Un lapsus! :)     VD       fr   \n",
       "3                          Dans le train pour Genève     VD       fr   \n",
       "4  Argh, pas de phó! (@ Goûts d'Asie) http://4sq....     VD       fr   \n",
       "\n",
       "                                        cleaned_text  Positive  Negative  \\\n",
       "0  un peu réconfort liquide take away après début...         1         0   \n",
       "1                   au charbon ! ( bcv st-françois )         0         0   \n",
       "2           c'est quoi laptop geek ? un lapsus ! : )         0         0   \n",
       "3                                  dans train genève         0         0   \n",
       "4                      argh , phó ! ( goûts d'asie )         0         0   \n",
       "\n",
       "   Anger  Anticipation  Disgust  Fear  Joy  Sadness  Surprise  Trust  \n",
       "0      0             0        0     0    0        0         0      1  \n",
       "1      0             0        0     0    0        0         0      0  \n",
       "2      0             0        0     0    0        0         0      0  \n",
       "3      0             0        0     0    0        0         0      0  \n",
       "4      0             0        0     0    0        0         0      0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_emptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions.to_csv('emotions_data/fr_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emotions=emotion_analysis('intermediate_data/de_cleaned.csv', 'German Translation (Google Translate)', 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emotions.to_csv('emotions_data/de_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emotions=emotion_analysis('intermediate_data/it_cleaned.csv', 'Italian Translation (Google Translate)', 'italian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emotions.to_csv('emotions_data/it_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/es_cleaned.csv', 'Spanish Translation (Google Translate)', 'spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions.to_csv('emotions_data/es_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# portuguese Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/pt_cleaned.csv', 'Portuguese Translation (Google Translate)', 'portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions.to_csv('emotions_data/pt_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/tr_cleaned.csv', 'Turkish Translation (Google Translate)', 'turkish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions.to_csv('emotions_data/tr_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/nl_cleaned.csv', 'Dutch Translation (Google Translate)', 'dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions.to_csv('emotions_data/nl_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions=emotion_analysis('intermediate_data/ar_cleaned.csv', 'Arabic Translation (Google Translate)', 'arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_with_emptions.to_csv('emotions_data/ar_final_emotion.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatnate all emotion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir(\"emotions_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar_final_emotion\n",
      "229191\n",
      "de_final_emotion\n",
      "1402170\n",
      "en_final_emotion\n",
      "2985994\n",
      "es_final_emotion\n",
      "443822\n",
      "fr_final_emotion\n",
      "3500272\n",
      "it_final_emotion\n",
      "484415\n",
      "nl_final_emotion\n",
      "197212\n",
      "pt_final_emotion\n",
      "476702\n",
      "tr_final_emotion\n",
      "331035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>canton</th>\n",
       "      <th>language</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.756668e+10</td>\n",
       "      <td>14331452.0</td>\n",
       "      <td>2010-07-02 12:05:23</td>\n",
       "      <td>@alfarhan خطيبنا تحدث عن علاقة الآباء بالابناء...</td>\n",
       "      <td>BE</td>\n",
       "      <td>ar</td>\n",
       "      <td>خطيبنا تحدث علاقة الآباء بالابناء كيف الاسلام ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.091966e+10</td>\n",
       "      <td>14331452.0</td>\n",
       "      <td>2010-08-11 21:52:55</td>\n",
       "      <td>للتو.. انتهينا من الراويح</td>\n",
       "      <td>BE</td>\n",
       "      <td>ar</td>\n",
       "      <td>للتو.. انتهينا الراويح</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.408070e+10</td>\n",
       "      <td>14331452.0</td>\n",
       "      <td>2010-09-10 05:47:05</td>\n",
       "      <td>في الطريق الى صلاة الجمعة.. تقبل الله منا و من...</td>\n",
       "      <td>BE</td>\n",
       "      <td>ar</td>\n",
       "      <td>الطريق صلاة الجمعة.. تقبل الله منا منكم صالح ا...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.408073e+10</td>\n",
       "      <td>14331452.0</td>\n",
       "      <td>2010-09-10 05:47:46</td>\n",
       "      <td>عفوا قصدت صلاة العيد</td>\n",
       "      <td>BE</td>\n",
       "      <td>ar</td>\n",
       "      <td>عفوا قصدت صلاة العيد</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.627465e+10</td>\n",
       "      <td>14331452.0</td>\n",
       "      <td>2010-10-03 15:12:59</td>\n",
       "      <td>افكار كثيرة لمشاريع تجول في خاطري لكن مع الاسف...</td>\n",
       "      <td>BE</td>\n",
       "      <td>ar</td>\n",
       "      <td>افكار كثيرة لمشاريع تجول خاطري الاسف الدراسة ا...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      userId            createdAt  \\\n",
       "0  1.756668e+10  14331452.0  2010-07-02 12:05:23   \n",
       "1  2.091966e+10  14331452.0  2010-08-11 21:52:55   \n",
       "2  2.408070e+10  14331452.0  2010-09-10 05:47:05   \n",
       "3  2.408073e+10  14331452.0  2010-09-10 05:47:46   \n",
       "4  2.627465e+10  14331452.0  2010-10-03 15:12:59   \n",
       "\n",
       "                                                text canton language  \\\n",
       "0  @alfarhan خطيبنا تحدث عن علاقة الآباء بالابناء...     BE       ar   \n",
       "1                          للتو.. انتهينا من الراويح     BE       ar   \n",
       "2  في الطريق الى صلاة الجمعة.. تقبل الله منا و من...     BE       ar   \n",
       "3                               عفوا قصدت صلاة العيد     BE       ar   \n",
       "4  افكار كثيرة لمشاريع تجول في خاطري لكن مع الاسف...     BE       ar   \n",
       "\n",
       "                                        cleaned_text  Positive  Negative  \\\n",
       "0  خطيبنا تحدث علاقة الآباء بالابناء كيف الاسلام ...         0         0   \n",
       "1                             للتو.. انتهينا الراويح         0         0   \n",
       "2  الطريق صلاة الجمعة.. تقبل الله منا منكم صالح ا...         2         0   \n",
       "3                               عفوا قصدت صلاة العيد         1         0   \n",
       "4  افكار كثيرة لمشاريع تجول خاطري الاسف الدراسة ا...         0         0   \n",
       "\n",
       "   Anger  Anticipation  Disgust  Fear  Joy  Sadness  Surprise  Trust  \n",
       "0      0             0        0     0    0        0         0      0  \n",
       "1      0             0        0     0    0        0         0      0  \n",
       "2      0             1        0     1    1        0         0      1  \n",
       "3      0             1        0     0    1        0         1      1  \n",
       "4      0             0        0     0    0        0         0      0  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "emotion_big_df=pd.DataFrame()\n",
    "i=0\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    name=file.split(\".\")[0]\n",
    "    print(name)\n",
    "    df=pd.read_csv(file)\n",
    "    print(len(df))\n",
    "    emotion_big_df=pd.concat([emotion_big_df,df], axis=0)\n",
    "emotion_big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10050813"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_big_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_big_df.to_csv('all_tweets_with_emotions.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
