{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from EmotionAnalysis.DataSchemaExtractionParsing import *\n",
    "from EmotionAnalysis.DataPreProcessing import *\n",
    "from EmotionAnalysis.SentSemanticModule import *\n",
    "from EmotionAnalysis.SentTweetModule import *\n",
    "from EmotionAnalysis.SentSyntacticModule import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook:\n",
    "In this notebook, we demonstrate the different steps followed in order to come up with a refined representation of each tweet by following two principles:\n",
    "* Word Qualification: application of stop word removal, part of speech tagging and named entity recognition and term normalization to keep good refined emotional candidates. \n",
    "* Inter-Word Relationships: Application of syntactic analysis to study three kinds of dependencies:\n",
    "    * Negation Dependency: e.g. I am not happy \n",
    "    * Adjectival Dependency: \n",
    "    * Adverbial Dependency\n",
    "    \n",
    "#### NB:\n",
    "This notebook makes direct calls to functions defined in EmotionAnalysis folder. Please refer to that in order to see details of implementation of different steps of the pipeline:\n",
    "* Pre processing\n",
    "* Syntactic Module\n",
    "* Semantic Word Level Module\n",
    "* Semantic Tweet Level Module\n",
    "\n",
    "We have runned the same code on the whole dataset chunk by chunk with several variations depending on the language and the libraries available for that specific language. But, for demonstration purposes, in this notebook, we show the process for a small subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading English Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>...</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>swiss</th>\n",
       "      <th>canton</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7198282.0</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town â at La Stanza...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Nico Luchsinger</td>\n",
       "      <td>halbluchs</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>ZH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9516952605</td>\n",
       "      <td>14703863.0</td>\n",
       "      <td>2010-02-23 07:51:47</td>\n",
       "      <td>Getting ready..  http://twitpic.com/14v8gz</td>\n",
       "      <td>8.81749</td>\n",
       "      <td>47.2288</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://stone.com/Twittelator</td>\n",
       "      <td>Urs</td>\n",
       "      <td>ugro</td>\n",
       "      <td>75.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>ZÃ¼rich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>SG</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9517916537</td>\n",
       "      <td>13535402.0</td>\n",
       "      <td>2010-02-23 08:35:39</td>\n",
       "      <td>I'm at Online PC Magazin in Adliswil http://go...</td>\n",
       "      <td>8.53010</td>\n",
       "      <td>47.3152</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Patrick Hediger</td>\n",
       "      <td>hediger</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>12157.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>ZH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9519149278</td>\n",
       "      <td>14260616.0</td>\n",
       "      <td>2010-02-23 09:32:09</td>\n",
       "      <td>@eyeem When and how can we send photos ? One p...</td>\n",
       "      <td>8.29953</td>\n",
       "      <td>47.4829</td>\n",
       "      <td>\\N</td>\n",
       "      <td>9518986782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>Roman Keller</td>\n",
       "      <td>RomanKeller</td>\n",
       "      <td>720.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>7337.0</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>AG</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9523488851</td>\n",
       "      <td>12391922.0</td>\n",
       "      <td>2010-02-23 12:30:04</td>\n",
       "      <td>I just ousted @keepthebyte as the mayor of Day...</td>\n",
       "      <td>7.59000</td>\n",
       "      <td>47.5550</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://foursquare.com</td>\n",
       "      <td>Gabriel Walt</td>\n",
       "      <td>GabrielWalt</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>Basel, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>BS</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      userId            createdAt  \\\n",
       "0  9514846412   7198282.0  2010-02-23 06:22:40   \n",
       "1  9516952605  14703863.0  2010-02-23 07:51:47   \n",
       "2  9517916537  13535402.0  2010-02-23 08:35:39   \n",
       "3  9519149278  14260616.0  2010-02-23 09:32:09   \n",
       "4  9523488851  12391922.0  2010-02-23 12:30:04   \n",
       "\n",
       "                                                text  longitude  latitude  \\\n",
       "0  Still the best coffee in town â at La Stanza...    8.53781   47.3678   \n",
       "1         Getting ready..  http://twitpic.com/14v8gz    8.81749   47.2288   \n",
       "2  I'm at Online PC Magazin in Adliswil http://go...    8.53010   47.3152   \n",
       "3  @eyeem When and how can we send photos ? One p...    8.29953   47.4829   \n",
       "4  I just ousted @keepthebyte as the mayor of Day...    7.59000   47.5550   \n",
       "\n",
       "  placeId   inReplyTo  source  truncated   ...     \\\n",
       "0      \\N          \\N   550.0        NaN   ...      \n",
       "1      \\N          \\N    62.0        NaN   ...      \n",
       "2      \\N          \\N   550.0        NaN   ...      \n",
       "3      \\N  9518986782     1.0        NaN   ...      \n",
       "4      \\N          \\N     3.0        NaN   ...      \n",
       "\n",
       "                               sourceUrl         userName   screenName  \\\n",
       "0                    http://gowalla.com/  Nico Luchsinger    halbluchs   \n",
       "1           http://stone.com/Twittelator              Urs         ugro   \n",
       "2                    http://gowalla.com/  Patrick Hediger      hediger   \n",
       "3  http://twitter.com/#!/download/iphone     Roman Keller  RomanKeller   \n",
       "4                  http://foursquare.com     Gabriel Walt  GabrielWalt   \n",
       "\n",
       "  followersCount friendsCount statusesCount          userLocation  swiss  \\\n",
       "0         1820.0        703.0        4687.0   Zurich, Switzerland    yes   \n",
       "1           75.0        161.0        1390.0  ZÃ¼rich, Switzerland    yes   \n",
       "2         1511.0        682.0       12157.0   Zurich, Switzerland    yes   \n",
       "3          720.0        821.0        7337.0           Switzerland    yes   \n",
       "4         1445.0       1627.0        1507.0    Basel, Switzerland    yes   \n",
       "\n",
       "   canton language  \n",
       "0      ZH       en  \n",
       "1      SG       en  \n",
       "2      ZH       en  \n",
       "3      AG       en  \n",
       "4      BS       en  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets = pd.read_csv(\"../../Data/Sample Data/en_sample.csv\",encoding = \"ISO-8859-1\",nrows=8000)\n",
    "english_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a tweet before applying any processing (we will use this example and several others to show how our steps refine an intermediary representation before applying any emotion recognition methodology):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wow so cool !!! http://twitpic.com/17ean4 - Golden Gate Bridge, San Francisco #hipstamatic #iphoneography #sanfrancisco /via @P_McBride'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets['text'].iloc[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Read & Learn about scalability!!! A brief interview with me about how we're using @cassandra at @twitter: http://bit.ly/bBadzO /via @rk\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets['text'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preliminary Pre-processing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Replacing Special Categories:\n",
    "We first start by dealing with some string patterns that are particular to the case of Twitter Data:\n",
    "* Urls: we detect and remove data as they don't carry any emotional importance\n",
    "* Digits: we remove them for the same reason\n",
    "* Detecting @ instances with <username> and removing it\n",
    "* Removing hashtag # sign and keeping the word after hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replaced_categories = handle_special_categories(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wow so cool !!!  - Golden Gate Bridge, San Francisco hipstamatic iphoneography sanfrancisco /via '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_categories['text'].iloc[160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice here that url, username and hashtag sign have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replacing contractions (needed for more accurate tokenization)\n",
    "e.g, \"they're stunning\" becomes \"they are stunning\". Otherwise, if we use tokenization right away, it will split into two words: they and 're and 're will cannot be removed since it is not part of stopwords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_no_contractions = replace_contractions(replaced_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Read & Learn about scalability!!! A brief interview with me about how we are using  at   /via '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_contractions['text'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here \"how we're\" has been replaced by \"how we are\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Tokenization of Tweets into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Good',\n",
       " u'morning',\n",
       " u'Black',\n",
       " u'Eyed',\n",
       " u'Peas',\n",
       " u'in',\n",
       " u'my',\n",
       " u'ears',\n",
       " u'finalizing',\n",
       " u'new',\n",
       " u'partnership',\n",
       " u'and',\n",
       " u'planning',\n",
       " u'some',\n",
       " u'upcoming',\n",
       " u'shoots',\n",
       " u'Busy',\n",
       " u'morning']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list = bag_of_word_representation(tweets_no_contractions)\n",
    "tokenized_list[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Syntactic Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Part of Speech Tagging:\n",
    "We use part of speech tagging here in order to detect N.A.V.A. words (Nouns, Adjectives, Verbs, Adverbs) those are good candidates to carry emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Good', 'JJ'),\n",
       " (u'morning', 'NN'),\n",
       " (u'Black', 'NNP'),\n",
       " (u'Eyed', 'NNP'),\n",
       " (u'Peas', 'NNP'),\n",
       " (u'in', 'IN'),\n",
       " (u'my', 'PRP$'),\n",
       " (u'ears', 'NNS'),\n",
       " (u'finalizing', 'VBG'),\n",
       " (u'new', 'JJ'),\n",
       " (u'partnership', 'NN'),\n",
       " (u'and', 'CC'),\n",
       " (u'planning', 'VBG'),\n",
       " (u'some', 'DT'),\n",
       " (u'upcoming', 'JJ'),\n",
       " (u'shoots', 'NNS'),\n",
       " (u'Busy', 'JJ'),\n",
       " (u'morning', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tweets = pos_tagging(tokenized_list)\n",
    "tagged_tweets[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dependency Parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STANFORD VERSION : More accurate but is too slow:\n",
    "import os\n",
    "os.environ[\"STANFORD_MODELS\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "os.environ[\"STANFORD_PARSER\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser=StanfordDependencyParser(model_path=\"/home/meryem/Downloads/stanford-parser-full-2016-10-31/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "dependency_trees = []\n",
    "tweets_list = tweets_no_contractions['text'][0:10]\n",
    "for i in range(0,len(tweets_list)):\n",
    "    trees = [parse.tree() for parse in dep_parser.raw_parse(tweets_list[i])]\n",
    "    result = dep_parser.raw_parse(tweets_list[i])\n",
    "    dep = result.next()\n",
    "    dependency_trees.append(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the sample data, we could not directly find interesting examples to show how our dependencies of interest (negation, adjectival complement and adverbial complement) are detected. That's why we will give a few examples that don't exist in the sample dataset but could be found in the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dependency_trees_examples = []\n",
    "trees_examples = []\n",
    "examples = ['I am not happy','What a bad luck','I am struggling happily']\n",
    "for i in range(0,len(examples)):\n",
    "    trees_examples.append([parse.tree() for parse in dep_parser.raw_parse(examples[i])])\n",
    "    result = dep_parser.raw_parse(examples[i])\n",
    "    dep = result.next()\n",
    "    dependency_trees_examples.append(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'happy', u'JJ'), u'nsubj', (u'I', u'PRP')),\n",
       " ((u'happy', u'JJ'), u'cop', (u'am', u'VBP')),\n",
       " ((u'happy', u'JJ'), u'neg', (u'not', u'RB'))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_trees_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here it has detected that happy depends on word not which cancels its emotion (happiness) as it has a negation dependency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'luck', u'NN'), u'dep', (u'What', u'WP')),\n",
       " ((u'luck', u'NN'), u'det', (u'a', u'DT')),\n",
       " ((u'luck', u'NN'), u'amod', (u'bad', u'JJ'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_trees_examples[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here it has detected that luck depends on word bad which cancels its emotion (positive) as it has an adjectival modifier (amod) dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'struggling', u'VBG'), u'nsubj', (u'I', u'PRP')),\n",
       " ((u'struggling', u'VBG'), u'aux', (u'am', u'VBP')),\n",
       " ((u'struggling', u'VBG'), u'advmod', (u'happily', u'RB'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_trees_examples[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here it has detected that struggling depends on word happily which cancels its emotion as it has an adjectival modifier (advmod) dependency. After that, struggling happily will have the emotion of depender \"happily\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION FOR DEPENDENCY PARSING:\n",
    "nlp = spacy.load('en') # Loading nlp pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating docs\n",
    "docs = []\n",
    "# Joining text:\n",
    "tweets_text = []\n",
    "for i in range(0, len(tokenized_list)):\n",
    "    space = u\" \"\n",
    "    tweets_text.append(space.join(tokenized_list[i]))\n",
    "tweets_text[0].encode(\"utf-8\")\n",
    "for i in range(0, len(tweets_text)):\n",
    "    doc = nlp(tweets_text[i])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples = []\n",
    "for sample in docs:\n",
    "    new_samples_sub = []\n",
    "    for word in sample:\n",
    "        new_samples_sub.append((unicode(word),word.pos_))\n",
    "    new_samples.append(new_samples_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application of Syntactic Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_samples_syn,triple_dependencies_syn = apply_syntactic_rules(docs,new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<< Original tweet text >>>\n",
      "\n",
      "Good morning Black Eyed Peas in my ears finalizing new partnership and planning some upcoming shoots Busy morning\n",
      "\n",
      "<<<< Tweet after applying syntactic Rules >>>\n",
      "\n",
      "[u'Good', u'Black', u'Eyed', u'Peas', u'in', u'my', u'ears', u'finalizing', u'new', u'and', u'planning', u'some', u'upcoming', u'shoots', u'Busy']\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print \"\\n<<<< Original tweet text >>>\\n\"\n",
    "print tweets_text[i]\n",
    "print \"\\n<<<< Tweet after applying syntactic Rules >>>\\n\"\n",
    "new_tweet = []\n",
    "for (word,pos) in new_samples[i]:\n",
    "    new_tweet.append(word)\n",
    "print new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Still', 'RB'), (u'the', 'DT'), (u'best', 'JJS'), (u'coffee', 'NN'), (u'in', 'IN'), (u'town', 'NN'), (u'at', 'IN'), (u'La', 'NNP'), (u'Stanza', 'NNP')]\n",
      "\n",
      "\n",
      "[(u'Still', u'ADV'), (u'the', u'DET'), (u'best', u'ADJ'), (u'in', u'ADP'), (u'town', u'NOUN'), (u'at', u'ADP'), (u'La', u'PROPN'), (u'Stanza', u'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_tweets[0]\n",
    "print \"\\n\"\n",
    "print new_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Further Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Named Entity Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "tweet_without_ne = remove_named_entities(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalizing POS tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Getting', 'v'), (u'ready', u'ADJ')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tags = normalize_pos_tags_words1(tweet_without_ne)\n",
    "normalized_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Removal of Punctuation and Stop words and Converting to Lower Case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_tweets_without = eliminate_stop_words_punct(normalized_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization:\n",
    "To normalize terms to one common version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'still', u'ADV'),\n",
       " (u'best', u'ADJ'),\n",
       " (u'town', 'n'),\n",
       " (u'la', u'PROPN'),\n",
       " (u'stanza', u'PROPN')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tweets = lemmatizer(tagged_tweets_without)\n",
    "\n",
    "lemmatized_tweets_untag = lemmatizer_untagged(tagged_tweets_without)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'get', 'v'), (u'ready', u'ADJ')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tweets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Keeping only NAVA words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'still', u'best', u'town']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nava_tweets = keep_only_nava_words(lemmatized_tweets)\n",
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lemmatizing Pre-cleaned Tokenized Tweets before any pre-processing:\n",
    "We need this in order to train word2vec model as it will be impacted by the relative distance between words and we need the same lemmatized lower case version in order to calculate similarity scores for words that exist in nava tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'still', u'the', u'best', u'coffee', u'in', u'town', u'at', u'la', u'stanza']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lemmatized = lemmatizer_raw(normalize_pos_tags_words1(tagged_tweets))\n",
    "tokenized_lemmatized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Saving the lemmatized tokenized version and nava version of Tweets in a dataframe for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nava_tweets_df = pd.DataFrame()\n",
    "nava_tweets_df['Tokenized Lemmatized'] = tokenized_lemmatized\n",
    "nava_tweets_df['Nava Representation'] = nava_tweets\n",
    "nava_tweets_df.to_csv('../../Results/Sample Affective Representation.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************* END *********************************************************"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
