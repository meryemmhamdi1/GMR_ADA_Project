{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from EmotionAnalysis.DataSchemaExtractionParsing import *\n",
    "from EmotionAnalysis.DataPreProcessing import *\n",
    "from EmotionAnalysis.SentSemanticModule import *\n",
    "from EmotionAnalysis.SentTweetModule import *\n",
    "from EmotionAnalysis.SentSyntacticModule import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this notebook:\n",
    "In this notebook, we demonstrate the different steps followed in order to come up with a refined representation of each tweet by following two principles:\n",
    "* Word Qualification: application of stop word removal, part of speech tagging and named entity recognition and term normalization to keep good refined emotional candidates. \n",
    "* Inter-Word Relationships: Application of syntactic analysis to study three kinds of dependencies:\n",
    "    * Negation Dependency: e.g. I am not happy \n",
    "    * Adjectival Dependency: \n",
    "    * Adverbial Dependency\n",
    "    \n",
    "#### NB:\n",
    "This notebook makes direct calls to functions defined in EmotionAnalysis folder. Please refer to that in order to see details of implementation of different steps of the pipeline:\n",
    "* Pre processing\n",
    "* Syntactic Module\n",
    "* Semantic Word Level Module\n",
    "* Semantic Tweet Level Module\n",
    "\n",
    "We have runned the same code on the whole dataset chunk by chunk with several variations depending on the language and the libraries available for that specific language. But, for demonstration purposes, in this notebook, we show the process for a small subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Loading English Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>...</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>swiss</th>\n",
       "      <th>canton</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7198282.0</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town — at La Stanza h...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Nico Luchsinger</td>\n",
       "      <td>halbluchs</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>ZH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9516952605</td>\n",
       "      <td>14703863.0</td>\n",
       "      <td>2010-02-23 07:51:47</td>\n",
       "      <td>Getting ready..  http://twitpic.com/14v8gz</td>\n",
       "      <td>8.81749</td>\n",
       "      <td>47.2288</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://stone.com/Twittelator</td>\n",
       "      <td>Urs</td>\n",
       "      <td>ugro</td>\n",
       "      <td>75.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>Zürich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>SG</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9517916537</td>\n",
       "      <td>13535402.0</td>\n",
       "      <td>2010-02-23 08:35:39</td>\n",
       "      <td>I'm at Online PC Magazin in Adliswil http://go...</td>\n",
       "      <td>8.53010</td>\n",
       "      <td>47.3152</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Patrick Hediger</td>\n",
       "      <td>hediger</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>12157.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>ZH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9519149278</td>\n",
       "      <td>14260616.0</td>\n",
       "      <td>2010-02-23 09:32:09</td>\n",
       "      <td>@eyeem When and how can we send photos ? One p...</td>\n",
       "      <td>8.29953</td>\n",
       "      <td>47.4829</td>\n",
       "      <td>\\N</td>\n",
       "      <td>9518986782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>Roman Keller</td>\n",
       "      <td>RomanKeller</td>\n",
       "      <td>720.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>7337.0</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>AG</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9523488851</td>\n",
       "      <td>12391922.0</td>\n",
       "      <td>2010-02-23 12:30:04</td>\n",
       "      <td>I just ousted @keepthebyte as the mayor of Day...</td>\n",
       "      <td>7.59000</td>\n",
       "      <td>47.5550</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://foursquare.com</td>\n",
       "      <td>Gabriel Walt</td>\n",
       "      <td>GabrielWalt</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>Basel, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>BS</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      userId            createdAt  \\\n",
       "0  9514846412   7198282.0  2010-02-23 06:22:40   \n",
       "1  9516952605  14703863.0  2010-02-23 07:51:47   \n",
       "2  9517916537  13535402.0  2010-02-23 08:35:39   \n",
       "3  9519149278  14260616.0  2010-02-23 09:32:09   \n",
       "4  9523488851  12391922.0  2010-02-23 12:30:04   \n",
       "\n",
       "                                                text  longitude  latitude  \\\n",
       "0  Still the best coffee in town — at La Stanza h...    8.53781   47.3678   \n",
       "1         Getting ready..  http://twitpic.com/14v8gz    8.81749   47.2288   \n",
       "2  I'm at Online PC Magazin in Adliswil http://go...    8.53010   47.3152   \n",
       "3  @eyeem When and how can we send photos ? One p...    8.29953   47.4829   \n",
       "4  I just ousted @keepthebyte as the mayor of Day...    7.59000   47.5550   \n",
       "\n",
       "  placeId   inReplyTo  source  truncated   ...     \\\n",
       "0      \\N          \\N   550.0        NaN   ...      \n",
       "1      \\N          \\N    62.0        NaN   ...      \n",
       "2      \\N          \\N   550.0        NaN   ...      \n",
       "3      \\N  9518986782     1.0        NaN   ...      \n",
       "4      \\N          \\N     3.0        NaN   ...      \n",
       "\n",
       "                               sourceUrl         userName   screenName  \\\n",
       "0                    http://gowalla.com/  Nico Luchsinger    halbluchs   \n",
       "1           http://stone.com/Twittelator              Urs         ugro   \n",
       "2                    http://gowalla.com/  Patrick Hediger      hediger   \n",
       "3  http://twitter.com/#!/download/iphone     Roman Keller  RomanKeller   \n",
       "4                  http://foursquare.com     Gabriel Walt  GabrielWalt   \n",
       "\n",
       "  followersCount friendsCount statusesCount         userLocation  swiss  \\\n",
       "0         1820.0        703.0        4687.0  Zurich, Switzerland    yes   \n",
       "1           75.0        161.0        1390.0  Zürich, Switzerland    yes   \n",
       "2         1511.0        682.0       12157.0  Zurich, Switzerland    yes   \n",
       "3          720.0        821.0        7337.0          Switzerland    yes   \n",
       "4         1445.0       1627.0        1507.0   Basel, Switzerland    yes   \n",
       "\n",
       "   canton language  \n",
       "0      ZH       en  \n",
       "1      SG       en  \n",
       "2      ZH       en  \n",
       "3      AG       en  \n",
       "4      BS       en  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets = pd.read_csv(\"../../Data/Sample Data/en_sample.csv\",encoding =\"utf-8\")\n",
    "english_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a tweet before applying any processing (we will use this example and several others to show how our steps refine an intermediary representation before applying any emotion recognition methodology):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wow so cool !!! http://twitpic.com/17ean4 - Golden Gate Bridge, San Francisco #hipstamatic #iphoneography #sanfrancisco /via @P_McBride'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets['text'].iloc[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Read & Learn about scalability!!! A brief interview with me about how we're using @cassandra at @twitter: http://bit.ly/bBadzO /via @rk\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets['text'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preliminary Pre-processing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Replacing Special Categories:\n",
    "We first start by dealing with some string patterns that are particular to the case of Twitter Data:\n",
    "* Urls: we detect and remove data as they don't carry any emotional importance\n",
    "* Digits: we remove them for the same reason\n",
    "* Detecting @ instances with <username> and removing it\n",
    "* Removing hashtag # sign and keeping the word after hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replaced_categories = handle_special_categories(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wow so cool !!!  - Golden Gate Bridge, San Francisco hipstamatic iphoneography sanfrancisco /via '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_categories['text'].iloc[160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice here that url, username and hashtag sign have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replacing contractions (needed for more accurate tokenization)\n",
    "e.g, \"they're stunning\" becomes \"they are stunning\". Otherwise, if we use tokenization right away, it will split into two words: they and 're and 're will cannot be removed since it is not part of stopwords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_no_contractions = replace_contractions(replaced_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Read & Learn about scalability!!! A brief interview with me about how we are using  at   /via '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_contractions['text'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here \"how we're\" has been replaced by \"how we are\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Tokenization of Tweets into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Good',\n",
       " u'morning',\n",
       " u'Black',\n",
       " u'Eyed',\n",
       " u'Peas',\n",
       " u'in',\n",
       " u'my',\n",
       " u'ears',\n",
       " u'finalizing',\n",
       " u'new',\n",
       " u'partnership',\n",
       " u'and',\n",
       " u'planning',\n",
       " u'some',\n",
       " u'upcoming',\n",
       " u'shoots',\n",
       " u'Busy',\n",
       " u'morning']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list = bag_of_word_representation(tweets_no_contractions)\n",
    "tokenized_list[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Syntactic Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Part of Speech Tagging:\n",
    "We use part of speech tagging here in order to detect N.A.V.A words (Nouns, Adjectives, Verbs, Adverbs) those are good candidates to carry emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Good', 'JJ'),\n",
       " (u'morning', 'NN'),\n",
       " (u'Black', 'NNP'),\n",
       " (u'Eyed', 'NNP'),\n",
       " (u'Peas', 'NNP'),\n",
       " (u'in', 'IN'),\n",
       " (u'my', 'PRP$'),\n",
       " (u'ears', 'NNS'),\n",
       " (u'finalizing', 'VBG'),\n",
       " (u'new', 'JJ'),\n",
       " (u'partnership', 'NN'),\n",
       " (u'and', 'CC'),\n",
       " (u'planning', 'VBG'),\n",
       " (u'some', 'DT'),\n",
       " (u'upcoming', 'JJ'),\n",
       " (u'shoots', 'NNS'),\n",
       " (u'Busy', 'JJ'),\n",
       " (u'morning', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tweets = pos_tagging(tokenized_list)\n",
    "tagged_tweets[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dependency Parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STANFORD VERSION : More accurate but is too slow:\n",
    "import os\n",
    "os.environ[\"STANFORD_MODELS\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "os.environ[\"STANFORD_PARSER\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser=StanfordDependencyParser(model_path=\"/home/meryem/Downloads/stanford-parser-full-2016-10-31/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "dependency_trees = []\n",
    "tweets_list = tweets_no_contractions['text']\n",
    "for i in range(0,len(tweets_list)):\n",
    "    trees = [parse.tree() for parse in dep_parser.raw_parse(tweets_list[i])]\n",
    "    result = dep_parser.raw_parse(tweets_list[i])\n",
    "    dep = result.next()\n",
    "    dependency_trees.append(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the sample data, we could not directly find interesting examples to show how our dependencies of interest (negation, adjectival complement and adverbial complement) are detected. That's why we will give a few examples that don't exist in the sample dataset but could be found in the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dependency_trees_examples = []\n",
    "trees_examples = []\n",
    "examples = ['I am not happy','What a bad luck','I am struggling happily']\n",
    "for i in range(0,len(examples)):\n",
    "    trees_examples.append([parse.tree() for parse in dep_parser.raw_parse(examples[i])])\n",
    "    result = dep_parser.raw_parse(examples[i])\n",
    "    dep = result.next()\n",
    "    dependency_trees_examples.append(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'happy', u'JJ'), u'nsubj', (u'I', u'PRP')),\n",
       " ((u'happy', u'JJ'), u'cop', (u'am', u'VBP')),\n",
       " ((u'happy', u'JJ'), u'neg', (u'not', u'RB'))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_trees_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here it has detected that happy depends on word not which cancels its emotion (happiness) as it has a negation dependency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'luck', u'NN'), u'dep', (u'What', u'WP')),\n",
       " ((u'luck', u'NN'), u'det', (u'a', u'DT')),\n",
       " ((u'luck', u'NN'), u'amod', (u'bad', u'JJ'))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_trees_examples[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here it has detected that luck depends on word bad which cancels its emotion (positive) as it has an adjectival modifier (amod) dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'struggling', u'VBG'), u'nsubj', (u'I', u'PRP')),\n",
       " ((u'struggling', u'VBG'), u'aux', (u'am', u'VBP')),\n",
       " ((u'struggling', u'VBG'), u'advmod', (u'happily', u'RB'))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_trees_examples[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here it has detected that struggling depends on word happily which cancels its emotion as it has an adjectival modifier (advmod) dependency. After that, struggling happily will have the emotion of depender \"happily\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION FOR DEPENDENCY PARSING:\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "# Joining text:\n",
    "tweets_text = []\n",
    "for i in range(0, len(tokenized_list)):\n",
    "    space = u\" \"\n",
    "    tweets_text.append(space.join(tokenized_list[i]))\n",
    "tweets_text[0].encode(\"utf-8\")\n",
    "for i in range(0, len(tweets_text)):\n",
    "    doc = nlp(tweets_text[i])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Still the best coffee in town at La Stanza"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples = []\n",
    "for sample in docs:\n",
    "    new_samples_sub = []\n",
    "    for word in sample:\n",
    "        new_samples_sub.append((unicode(word),word.pos_))\n",
    "    new_samples.append(new_samples_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Still', u'ADV'),\n",
       " (u'the', u'DET'),\n",
       " (u'best', u'ADJ'),\n",
       " (u'coffee', u'NOUN'),\n",
       " (u'in', u'ADP'),\n",
       " (u'town', u'NOUN'),\n",
       " (u'at', u'ADP'),\n",
       " (u'La', u'PROPN'),\n",
       " (u'Stanza', u'PROPN')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Syntactic Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\xfc' in position 3: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-216ba538d523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtriple_dependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_syntactic_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/media/diskD/EPFL/Fall 2016/ADA/Project/GMR_ADA_Project/Notebooks/II. Polarity and Emotion Recognition/EmotionAnalysis/SentSyntacticModule.pyc\u001b[0m in \u001b[0;36mapply_syntactic_rules\u001b[1;34m(docs, new_samples)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Looking for negation dependency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mnew_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0mnew_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mVERB\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xfc' in position 3: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "new_samples,triple_dependencies = apply_syntactic_rules(docs,new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<< Original tweet text >>>\n",
      "\n",
      "So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits If it is unobserved it will however if it is observed after it is left the plane but before it hits its target it will not have gone through both slits\n",
      "\n",
      "<<<< Syntactic dependencies >>>\n",
      "\n",
      "[(So, u'advmod', observed), (if, u'mark', directed), (a, u'det', photon), (photon, u'nsubjpass', directed), (is, u'auxpass', directed), (directed, u'advcl', observed), (through, u'prep', directed), (a, u'det', plane), (plane, u'pobj', through), (with, u'prep', directed), (two, u'nummod', slits), (slits, u'pobj', with), (in, u'prep', slits), (it, u'pobj', in), (and, u'cc', directed), (either, u'advmod', slit), (slit, u'conj', directed), (is, u'auxpass', observed), (observed, u'ROOT', observed), (it, u'nsubj', go), (will, u'aux', go), (not, u'neg', go), (go, u'ccomp', observed), (through, u'prep', go), (both, u'det', slits), (slits, u'pobj', through), (If, u'mark', is), (it, u'nsubj', is), (is, u'advcl', observed), (unobserved, u'acomp', is), (it, u'nsubjpass', observed), (will, u'aux', observed), (however, u'advmod', observed), (if, u'mark', observed), (it, u'nsubjpass', observed), (is, u'auxpass', observed), (observed, u'ROOT', observed), (after, u'mark', left), (it, u'nsubjpass', left), (is, u'auxpass', left), (left, u'advcl', observed), (the, u'det', plane), (plane, u'dobj', left), (but, u'cc', left), (before, u'mark', hits), (it, u'nsubj', hits), (hits, u'advcl', gone), (its, u'poss', target), (target, u'dobj', hits), (it, u'nsubj', gone), (will, u'aux', gone), (not, u'neg', gone), (have, u'aux', gone), (gone, u'ccomp', observed), (through, u'prep', gone), (both, u'det', slits), (slits, u'pobj', through)]\n",
      "\n",
      "<<<< Tweet after applying syntactic Rules >>>\n",
      "\n",
      "[u'if', u'a', u'photon', u'directed', u'through', u'a', u'plane', u'with', u'two', u'slits', u'in', u'it', u'and', u'slit', u'observed', u'it', u'will', u'through', u'both', u'slits', u'If', u'it', u'unobserved', u'it', u'will', u'if', u'it', u'observed', u'after', u'it', u'left', u'the', u'plane', u'but', u'before', u'it', u'hits', u'its', u'target', u'it', u'will', u'have', u'through', u'both', u'slits', 'not_go', 'not_gone']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print \"\\n<<<< Original tweet text >>>\\n\"\n",
    "print tweets_text[i]\n",
    "print \"\\n<<<< Syntactic dependencies >>>\\n\"\n",
    "print triple_dependencies[i]\n",
    "print \"\\n<<<< Tweet after applying syntactic Rules >>>\\n\"\n",
    "new_tweet = []\n",
    "for (word,pos) in new_samples[i]:\n",
    "    new_tweet.append(word)\n",
    "print new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Still', 'RB'), (u'the', 'DT'), (u'best', 'JJS'), (u'coffee', 'NN'), (u'in', 'IN'), (u'town', 'NN'), (u'at', 'IN'), (u'La', 'NNP'), (u'Stanza', 'NNP')]\n",
      "\n",
      "\n",
      "[(u'the', u'DET'), (u'best', u'ADJ'), (u'in', u'ADP'), (u'town', u'NOUN'), (u'at', u'ADP'), (u'La', u'PROPN'), (u'Stanza', u'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_tweets[0]\n",
    "print \"\\n\"\n",
    "print new_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_without_ne = remove_named_entities(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ready', u'ADJ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tags = normalize_pos_tags_words(tweet_without_ne)\n",
    "normalized_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation and Stop words and Converting to Lower Case and Removal of Other special categories: url, number, username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_tweets_without = eliminate_stop_words_punct(normalized_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'best', u'ADJ'), (u'town', 'n'), (u'la', u'PROPN'), (u'stanza', u'PROPN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tweets = lemmatizer(tagged_tweets_without)\n",
    "\n",
    "lemmatized_tweets_untag = lemmatizer_untagged(tagged_tweets_without)\n",
    "lemmatized_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only NAVA words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nava_tweets = keep_only_nava_words(lemmatized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'best', u'town']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nava_tweets_df = pd.DataFrame()\n",
    "nava_tweets_df['Nava Tweets'] = nava_tweets\n",
    "nava_tweets_df.to_csv('Results/nava representation.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting NRC Lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE THIS TO DIRECTLY LOAD THE PROCESSED LEXICON\n",
    "lexicon_df = pd.read_csv('NRCLexicon/lexicon_nrc.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon = extractLexicon()\n",
    "word_set = list(set(lexicon['Word']))\n",
    "word_emotional_vectors = []\n",
    "for word in word_set:\n",
    "    word_emotional_vectors.append((word,list(lexicon[lexicon['Word']==word]['Score'])))\n",
    "word_emotional_vectors_dict = dict(word_emotional_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '0', '0', '1', '0', '1', '0', '0', '1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.head(1)\n",
    "word_emotional_vectors_dict['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    5: 'Negative',\n",
    "    6: 'Positive',\n",
    "    7: 'Sadness',\n",
    "    8: 'Surprise',\n",
    "    9: 'Trust',\n",
    "    10: 'Neutral'\n",
    "}\n",
    "emotion_ids = []\n",
    "for word in word_emotional_vectors_dict.keys():\n",
    "    for i in range(0,len(word_emotional_vectors_dict[word])):\n",
    "        if word_emotional_vectors_dict[word][i] == '1':\n",
    "            emotion_ids.append((word,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3324"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_repres_words_list = [] \n",
    "for i in range(0,10):\n",
    "    emotion_repres_words_list_sub = []\n",
    "    for (word,emotion) in emotion_ids:\n",
    "        if emotion == i:\n",
    "            emotion_repres_words_list_sub.append(word)\n",
    "    emotion_repres_words_list.append(emotion_repres_words_list_sub)\n",
    "len(emotion_repres_words_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon_df = pd.DataFrame()\n",
    "lexicon_df[0] = emotion_repres_words_list[0]\n",
    "for i in range(1,10):\n",
    "    df = pd.DataFrame()\n",
    "    df[i] = emotion_repres_words_list[i]\n",
    "    lexicon_df= pd.concat([lexicon_df,df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon_df.to_csv('NRCLexicon/lexicon_nrc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lexicon_df = pd.read_csv('NRCLexicon/lexicon_nrc.csv')\n",
    "lexicon_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_lexicon = make_unique_lexicon(lexicon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Semantic Similarity using PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-7ba784b9a36d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflatten_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmatized_tweets\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "flatten_list = [word for sublist in lemmatized_tweets for (word, tag) in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_pmi_dict = calculate_pmi(flatten_list,unique_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_pmi_based = compute_matrix_sentences_list(nava_tweets,lexicon_df, clean_pmi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    #5: 'Negative',\n",
    "    #6: 'Positive',\n",
    "    5: 'Sadness',\n",
    "    6: 'Surprise',\n",
    "    7: 'Trust',\n",
    "    8: 'Neutral'\n",
    "}\n",
    "sent_dict = {\n",
    "    0: \"Positive\",\n",
    "    1: \"Negative\",\n",
    "    2: \"Neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emotion Recognition\n",
    "sentence_vectors_pmi = compute_sentence_emotion_vectors(emotion_pmi_based)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_pmi)\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentence_vectors_sent_pmi = compute_sentence_sentiment_vectors(emotion_pmi_based)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['Affective Feature Representation'] = lemmatized_tweets\n",
    "english_tweets['Emotion Ids'] = sentence_vectors_pmi\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])\n",
    "english_tweets['Emotionalities'] = emotions\n",
    "english_tweets['Sentiments'] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets_col = english_tweets[['text','Affective Feature Representation','Emotion Ids','Emotionalities','Sentiments']]\n",
    "english_tweets_col.to_csv('PMI_Lexicon_ResultsSampleEnglishData.csv',encoding =\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Emotionalities'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(9)\n",
    "cs=cm.Set1(np.arange(9)/9.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Fine-grained Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Sentiments'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(3)\n",
    "cs=cm.Set1(np.arange(3)/3.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Sentiment Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Same approach using word2vec similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training Word2Vec Model on the whole tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_tokenized_lemma = lemmatizer_raw(tagged_tweets)\n",
    "len(raw_tokenized_lemma)\n",
    "raw_tokenized_lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = train_word2Vec_model(300, 40, 4, 10, 1e-3 , raw_tokenized_lemma, \"geo_tweets_word2vec_model\"): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.similarity('happy','so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_word2vec_based = compute_matrix_sentences_list_word2vec(lemmatized_tweets,lexicon_df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotion_word2vec_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Emotion Recognition\n",
    "sentence_vectors_word2vec = compute_sentence_emotion_vectors(emotion_word2vec_based)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_word2vec)\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentence_vectors_sent_word2vec = compute_sentence_sentiment_vectors(emotion_word2vec_based)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['Affective Feature Representation'] = lemmatized_tweets\n",
    "english_tweets['Emotion Ids'] = sentence_vectors_pmi\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])\n",
    "english_tweets['Emotionalities'] = emotions\n",
    "english_tweets['Sentiments'] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_tweets_col = english_tweets[['text','Affective Feature Representation','Emotion Ids','Emotionalities','Sentiments']]\n",
    "english_tweets_col.to_csv('Word2Vec_Lexicon_ResultsSampleEnglishData.csv',encoding =\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Emotionalities'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(9)\n",
    "cs=cm.Set1(np.arange(9)/9.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Fine-grained Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Sentiments'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(3)\n",
    "cs=cm.Set1(np.arange(3)/3.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Sentiment Emotion Distribution\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
