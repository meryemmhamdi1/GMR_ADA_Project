{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from EmotionAnalysis.SentSemanticModule import *\n",
    "from EmotionAnalysis.SentTweetModule import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Training Word2Vec Model for Multiple Languages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Tokenized Lemmatized Representation before further cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'still', u'the', u'best', u'coffee', u'in', u'town', u'at', u'la', u'stanza']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Tokenized Lemmatized\n",
    "tokenized_df = pd.read_csv('../../Results/Sample Affective Representation.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Converting them into a list\n",
    "tokenized_lemma = list(tokenized_df['Tokenized Lemmatized'])\n",
    "tokenized_lemmatized_tweets = []\n",
    "for i in range(0, len(tokenized_lemma)):\n",
    "    result = ast.literal_eval(tokenized_lemma[i])\n",
    "    tokenized_lemmatized_tweets.append(result)\n",
    "tokenized_lemmatized_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training and Fine-Tuning Word Embedding Model using gensim word2vec: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS TO BE TUNED:\n",
    "\n",
    "# Word vector dimensionality                      \n",
    "# Minimum word count                        \n",
    "# Number of threads to run in parallel\n",
    "# Context window size                                                                                    \n",
    "# Downsample setting for frequent words\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 4  # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(tokenized_lemmatized_tweets, workers=num_workers,\n",
    "            size=num_features, min_count = min_word_count,\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"../../Models/en_sample\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Applying Rule Based Lexicon Approach Extended with Word2Vec Methodology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Tweets in their Affective Representation form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'still', u'best', u'town']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nava_repr = list(tokenized_df['Nava Representation'])\n",
    "# Convert nava_tweets \n",
    "nava_tweets = []\n",
    "for i in range(0, len(nava_repr)):\n",
    "    result = ast.literal_eval(nava_repr[i])\n",
    "    nava_tweets.append(result)\n",
    "nava_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Language Lexicon (version containing set of representative words): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cussed</td>\n",
       "      <td>conjure</td>\n",
       "      <td>foul</td>\n",
       "      <td>foul</td>\n",
       "      <td>tantalizing</td>\n",
       "      <td>fawn</td>\n",
       "      <td>conformance</td>\n",
       "      <td>scold</td>\n",
       "      <td>conjure</td>\n",
       "      <td>digit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foul</td>\n",
       "      <td>immature</td>\n",
       "      <td>scold</td>\n",
       "      <td>aggression</td>\n",
       "      <td>elegant</td>\n",
       "      <td>inadequacy</td>\n",
       "      <td>eligible</td>\n",
       "      <td>dissolution</td>\n",
       "      <td>originality</td>\n",
       "      <td>admiral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aggression</td>\n",
       "      <td>tantalizing</td>\n",
       "      <td>screaming</td>\n",
       "      <td>scold</td>\n",
       "      <td>buddy</td>\n",
       "      <td>foul</td>\n",
       "      <td>electricity</td>\n",
       "      <td>cytomegalovirus</td>\n",
       "      <td>dissolution</td>\n",
       "      <td>specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scold</td>\n",
       "      <td>buddy</td>\n",
       "      <td>hanging</td>\n",
       "      <td>dissolution</td>\n",
       "      <td>oasis</td>\n",
       "      <td>narcotic</td>\n",
       "      <td>originality</td>\n",
       "      <td>hanging</td>\n",
       "      <td>tantalizing</td>\n",
       "      <td>reporter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dissolution</td>\n",
       "      <td>oasis</td>\n",
       "      <td>loathing</td>\n",
       "      <td>screaming</td>\n",
       "      <td>symphony</td>\n",
       "      <td>conjuring</td>\n",
       "      <td>tantalizing</td>\n",
       "      <td>sterile</td>\n",
       "      <td>shriek</td>\n",
       "      <td>buddy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1          2            3            4           5  \\\n",
       "0       cussed      conjure       foul         foul  tantalizing        fawn   \n",
       "1         foul     immature      scold   aggression      elegant  inadequacy   \n",
       "2   aggression  tantalizing  screaming        scold        buddy        foul   \n",
       "3        scold        buddy    hanging  dissolution        oasis    narcotic   \n",
       "4  dissolution        oasis   loathing    screaming     symphony   conjuring   \n",
       "\n",
       "             6                7            8           9  \n",
       "0  conformance            scold      conjure       digit  \n",
       "1     eligible      dissolution  originality     admiral  \n",
       "2  electricity  cytomegalovirus  dissolution  specialist  \n",
       "3  originality          hanging  tantalizing    reporter  \n",
       "4  tantalizing          sterile       shriek       buddy  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_df = pd.read_csv('../../NRCLexicon/English/lexicon_nrc.csv',encoding='ISO-8859-1')\n",
    "lexicon_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading Word2Vec Model (if it was already Pre-trained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec ....\n"
     ]
    }
   ],
   "source": [
    "print (\"Loading Word2Vec ....\")\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load('../../Models/en_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_matrix_sentences_list_word2vec(nava_words, nrc_lexicon,model):\n",
    "    \"\"\"\n",
    "\n",
    "    :param word2vec model:\n",
    "    :param nava_words: we can pass any version of the bag of words\n",
    "    :param nrc_lexicon:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    sm_list = list_nrc_lexicon(nrc_lexicon)\n",
    "    emotions = nrc_lexicon.columns.values\n",
    "    matrix_sentences_list = []\n",
    "    for i in range(0, len(nava_words)): # Iterate over all sentences\n",
    "        \" Initialize matrix for each sentence \"\n",
    "        w, h = len(nava_words[i]), 10\n",
    "        matrix_sentence = [[0 for x in range(w)] for y in range(h)]\n",
    "        k = 0\n",
    "        for word in nava_words[i]: # Iterate over all words in the sentence\n",
    "            j = 0\n",
    "            for emotion in range(0, len(emotions)): # Iterate over all emotions => fill in the emotional vectors for all words\n",
    "                total_similarity = 0\n",
    "                for representative_word in sm_list[emotion]:\n",
    "                    r = len(sm_list[emotion])\n",
    "                    if word in model and representative_word in model:\n",
    "                        total_similarity += model.similarity(word, representative_word)\n",
    "                matrix_sentence[j][k] += total_similarity / r \n",
    "                j += 1 # increment index of representative words\n",
    "            k += 1 # increment index of transcript words\n",
    "        # append the matrix_sentence to the global list for all sentences\n",
    "        matrix_sentences_list.append(matrix_sentence)\n",
    "    return matrix_sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Computing Word Level Emotional Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing word level scores\n"
     ]
    }
   ],
   "source": [
    "###### STEP 4: Word Level\n",
    "print (\"Computing word level scores\")\n",
    "matrix_sentences_word2vec = compute_matrix_sentences_list_word2vec(nava_tweets,lexicon_df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def compute_sentence_emotion_vectors(matrix_sentences_list):\n",
    "    emotion_vector_list = []\n",
    "    for i in range(0, len(matrix_sentences_list)):\n",
    "        sum_sentence = []\n",
    "        ids = [0,1,2,3,4,7,8,9]\n",
    "        for j in ids: # for each emotion\n",
    "            sum_words = 0\n",
    "            for k in range(0, len(matrix_sentences_list[i][j])):\n",
    "                sum_words += matrix_sentences_list[i][j][k]*1000\n",
    "            r = len(matrix_sentences_list[i])\n",
    "            if r != 0 :\n",
    "                sum_words = sum_words / r # Arithmetic mean\n",
    "            sum_sentence.append(sum_words)\n",
    "        emotion_vector_list.append(sum_sentence)\n",
    "    return emotion_vector_list\n",
    "\n",
    "def compute_sentence_sentiment_vectors(matrix_sentences_list):\n",
    "    emotion_vector_list = []\n",
    "    for i in range(0, len(matrix_sentences_list)):\n",
    "        sum_sentence = []\n",
    "        ids = [5,6]\n",
    "        for j in ids: # for each emotion\n",
    "            sum_words = 0\n",
    "            for k in range(0, len(matrix_sentences_list[i][j])):\n",
    "                sum_words += matrix_sentences_list[i][j][k]*1000\n",
    "            r = len(matrix_sentences_list[i])\n",
    "            if r != 0 : \n",
    "                sum_words = sum_words / r\n",
    "            sum_sentence.append(sum_words)\n",
    "        emotion_vector_list.append(sum_sentence)\n",
    "    return emotion_vector_list\n",
    "\n",
    "def compute_emotionalities(sentence_vectors):\n",
    "    emotionalities = []\n",
    "    threshold = 0 # THRESHOLD PARAMETER TO BE FINE TUNED (0 for lexicon, 0.2 for pmi)\n",
    "    for i in range(0,len(sentence_vectors)):\n",
    "        sentence_vector = sentence_vectors[i]\n",
    "        mylist = [0 if math.isnan(x) else x for x in sentence_vector]\n",
    "        if (max(mylist) > threshold): #Threshold \n",
    "            emotionalities.append(sentence_vectors[i].index(max(mylist)))\n",
    "        else: \n",
    "            emotionalities.append(8)\n",
    "    return emotionalities\n",
    "\n",
    "def compute_sentiments(sentence_vectors_sent,emotionalities):\n",
    "    sentiments = []\n",
    "    threshold = 0 # THRESHOLD PARAMETER TO BE FINE TUNED (0 for lexicon, 0.2 for pmi)\n",
    "    for i in range(0,len(sentence_vectors_sent)):\n",
    "        sentence_vector = sentence_vectors_sent[i]\n",
    "        mylist = [0 if math.isnan(x) else x for x in sentence_vector]\n",
    "        if (max(mylist) > threshold): #Threshold \n",
    "            sentiments.append(sentence_vectors_sent[i].index(max(mylist)))\n",
    "        else:\n",
    "            # To increase Recall, we also use emotionalities, in case a tweet is neutral\n",
    "            if emotionalities[i] in [0,2,3,5]:\n",
    "                sentiments.append(0) # Negative Emotion\n",
    "            if emotionalities[i] in [1,4,6,7]:\n",
    "                sentiments.append(1) # Positive Emotion\n",
    "            if emotionalities[i] == 8:\n",
    "                sentiments.append(2) # Otherwise, we just return Neutral\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Computing Tweet Level Emotionalities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Emotionalities ...\n",
      "\n",
      "Emotional Vectors >>>>\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.86966970083820316, 0.25169308940941049, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.57946677978576888, 0.16778532842800961, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.8695853898720538, 0.25174701455266491, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.20845536238516868, 0.058964488861077233, 0.0, 0.0]]\n",
      "\n",
      "\n",
      "Dominant Emotions Ids >>>>\n",
      "[4, 4, 8, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print (\"Computing Emotionalities ...\")\n",
    "# Computing vectors of emotional scores by averaging over the word emotion scores\n",
    "sentence_vectors_word2vec = compute_sentence_emotion_vectors(matrix_sentences_word2vec)\n",
    "\n",
    "print \"\\nEmotional Vectors >>>>\"\n",
    "print sentence_vectors_word2vec[0:5]\n",
    "print \"\\n\"\n",
    "# Selecting dominant emotion using a specific threshold\n",
    "emotionalities = compute_emotionalities(sentence_vectors_word2vec)\n",
    "\n",
    "print \"Dominant Emotions Ids >>>>\"\n",
    "print emotionalities[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Computing Tweet Level Sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentiments ...\n",
      "\n",
      "Sentiment Vectors >>>>\n",
      "[[0.0, 0.0], [0.0, 0.0]]\n",
      "\n",
      "\n",
      "Dominant Sentiments Ids >>>>\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "print (\"Computing sentiments ...\")\n",
    "\n",
    "# Computing vectors of polarity scores by averaging over the word polarity scores\n",
    "sentence_vectors_sent = compute_sentence_sentiment_vectors(matrix_sentences_word2vec)\n",
    "\n",
    "print \"\\nSentiment Vectors >>>>\"\n",
    "print sentence_vectors_sent[0:2]\n",
    "print \"\\n\"\n",
    "\n",
    "# Selecting dominant polarity (positive, negative) using a specific threshold\n",
    "sentiments = compute_sentiments(sentence_vectors_sent,emotionalities)\n",
    "\n",
    "print \"Dominant Sentiments Ids >>>>\"\n",
    "print sentiments[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Storing Emotion + Sentiment for each tweet in two lists to be used \n",
    "# for storing in dataframe:\n",
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    5: 'Sadness',\n",
    "    6: 'Surprise',\n",
    "    7: 'Trust',\n",
    "    8: 'Neutral'\n",
    "}\n",
    "sent_dict = {\n",
    "    0: \"Negative\",\n",
    "    1: \"Positive\",\n",
    "    2: \"Neutral\"\n",
    "}\n",
    "\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Storing emotion, sentiment and their score vectors in dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing in dataframe ... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nava Tweet</th>\n",
       "      <th>Emotion Vectors</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment Vectors</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[still, best, town]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.869669700838, 0.2516930...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[get, ready]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.579466779786, 0.1677853...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[when, send, photo]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.869585389872, 0.2517470...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[oust, mayor]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.208455362385, 0.0589644...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Nava Tweet                                    Emotion Vectors  \\\n",
       "0  [still, best, town]  [0.0, 0.0, 0.0, 0.0, 0.869669700838, 0.2516930...   \n",
       "1         [get, ready]  [0.0, 0.0, 0.0, 0.0, 0.579466779786, 0.1677853...   \n",
       "2                   []           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "3  [when, send, photo]  [0.0, 0.0, 0.0, 0.0, 0.869585389872, 0.2517470...   \n",
       "4        [oust, mayor]  [0.0, 0.0, 0.0, 0.0, 0.208455362385, 0.0589644...   \n",
       "\n",
       "   Emotion Sentiment Vectors Sentiment  \n",
       "0      Joy        [0.0, 0.0]  Positive  \n",
       "1      Joy        [0.0, 0.0]  Positive  \n",
       "2  Neutral        [0.0, 0.0]   Neutral  \n",
       "3      Joy        [0.0, 0.0]  Positive  \n",
       "4      Joy        [0.0, 0.0]  Positive  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Storing in dataframe ... \")\n",
    "word2vec_results_df = pd.DataFrame()\n",
    "\n",
    "word2vec_results_df['Nava Tweet'] = nava_tweets\n",
    "\n",
    "word2vec_results_df['Emotion Vectors'] = sentence_vectors_word2vec\n",
    "\n",
    "word2vec_results_df['Emotion'] = emotions\n",
    "\n",
    "word2vec_results_df['Sentiment Vectors'] = sentence_vectors_sent\n",
    "\n",
    "word2vec_results_df['Sentiment'] = senti\n",
    "\n",
    "word2vec_results_df.to_csv('../../Results/Sample Tweets Labelled Word2Vec.csv')\n",
    "word2vec_results_df.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
