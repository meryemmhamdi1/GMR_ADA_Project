{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, \"/media/diskD/EPFL/Fall 2016/ADA/Project/GMR_ADA_Project/EmotionAnalysis\")\n",
    "from DataSchemaExtractionParsing import *\n",
    "from DataPreProcessing import *\n",
    "from SentSemanticModule import *\n",
    "from SentTweetModule import *\n",
    "from SentSyntacticModule import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading English Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>...</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>swiss</th>\n",
       "      <th>canton</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9514846412</td>\n",
       "      <td>7198282.0</td>\n",
       "      <td>2010-02-23 06:22:40</td>\n",
       "      <td>Still the best coffee in town — at La Stanza h...</td>\n",
       "      <td>8.53781</td>\n",
       "      <td>47.3678</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Nico Luchsinger</td>\n",
       "      <td>halbluchs</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>4687.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>ZH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9516952605</td>\n",
       "      <td>14703863.0</td>\n",
       "      <td>2010-02-23 07:51:47</td>\n",
       "      <td>Getting ready..  http://twitpic.com/14v8gz</td>\n",
       "      <td>8.81749</td>\n",
       "      <td>47.2288</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://stone.com/Twittelator</td>\n",
       "      <td>Urs</td>\n",
       "      <td>ugro</td>\n",
       "      <td>75.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>Zürich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>SG</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9517916537</td>\n",
       "      <td>13535402.0</td>\n",
       "      <td>2010-02-23 08:35:39</td>\n",
       "      <td>I'm at Online PC Magazin in Adliswil http://go...</td>\n",
       "      <td>8.53010</td>\n",
       "      <td>47.3152</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://gowalla.com/</td>\n",
       "      <td>Patrick Hediger</td>\n",
       "      <td>hediger</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>12157.0</td>\n",
       "      <td>Zurich, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>ZH</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9519149278</td>\n",
       "      <td>14260616.0</td>\n",
       "      <td>2010-02-23 09:32:09</td>\n",
       "      <td>@eyeem When and how can we send photos ? One p...</td>\n",
       "      <td>8.29953</td>\n",
       "      <td>47.4829</td>\n",
       "      <td>\\N</td>\n",
       "      <td>9518986782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>Roman Keller</td>\n",
       "      <td>RomanKeller</td>\n",
       "      <td>720.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>7337.0</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>AG</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9523488851</td>\n",
       "      <td>12391922.0</td>\n",
       "      <td>2010-02-23 12:30:04</td>\n",
       "      <td>I just ousted @keepthebyte as the mayor of Day...</td>\n",
       "      <td>7.59000</td>\n",
       "      <td>47.5550</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>http://foursquare.com</td>\n",
       "      <td>Gabriel Walt</td>\n",
       "      <td>GabrielWalt</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>Basel, Switzerland</td>\n",
       "      <td>yes</td>\n",
       "      <td>BS</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      userId            createdAt  \\\n",
       "0  9514846412   7198282.0  2010-02-23 06:22:40   \n",
       "1  9516952605  14703863.0  2010-02-23 07:51:47   \n",
       "2  9517916537  13535402.0  2010-02-23 08:35:39   \n",
       "3  9519149278  14260616.0  2010-02-23 09:32:09   \n",
       "4  9523488851  12391922.0  2010-02-23 12:30:04   \n",
       "\n",
       "                                                text  longitude  latitude  \\\n",
       "0  Still the best coffee in town — at La Stanza h...    8.53781   47.3678   \n",
       "1         Getting ready..  http://twitpic.com/14v8gz    8.81749   47.2288   \n",
       "2  I'm at Online PC Magazin in Adliswil http://go...    8.53010   47.3152   \n",
       "3  @eyeem When and how can we send photos ? One p...    8.29953   47.4829   \n",
       "4  I just ousted @keepthebyte as the mayor of Day...    7.59000   47.5550   \n",
       "\n",
       "  placeId   inReplyTo  source  truncated   ...     \\\n",
       "0      \\N          \\N   550.0        NaN   ...      \n",
       "1      \\N          \\N    62.0        NaN   ...      \n",
       "2      \\N          \\N   550.0        NaN   ...      \n",
       "3      \\N  9518986782     1.0        NaN   ...      \n",
       "4      \\N          \\N     3.0        NaN   ...      \n",
       "\n",
       "                               sourceUrl         userName   screenName  \\\n",
       "0                    http://gowalla.com/  Nico Luchsinger    halbluchs   \n",
       "1           http://stone.com/Twittelator              Urs         ugro   \n",
       "2                    http://gowalla.com/  Patrick Hediger      hediger   \n",
       "3  http://twitter.com/#!/download/iphone     Roman Keller  RomanKeller   \n",
       "4                  http://foursquare.com     Gabriel Walt  GabrielWalt   \n",
       "\n",
       "  followersCount friendsCount statusesCount         userLocation  swiss  \\\n",
       "0         1820.0        703.0        4687.0  Zurich, Switzerland    yes   \n",
       "1           75.0        161.0        1390.0  Zürich, Switzerland    yes   \n",
       "2         1511.0        682.0       12157.0  Zurich, Switzerland    yes   \n",
       "3          720.0        821.0        7337.0          Switzerland    yes   \n",
       "4         1445.0       1627.0        1507.0   Basel, Switzerland    yes   \n",
       "\n",
       "   canton language  \n",
       "0      ZH       en  \n",
       "1      SG       en  \n",
       "2      ZH       en  \n",
       "3      AG       en  \n",
       "4      BS       en  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets = pd.read_csv(\"Data/en_sample.csv\",encoding =\"utf-8\")\n",
    "english_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'userId', u'createdAt', u'text', u'longitude', u'latitude',\n",
       "       u'placeId', u'inReplyTo', u'source', u'truncated', u'placeLatitude',\n",
       "       u'placeLongitude', u'sourceName', u'sourceUrl', u'userName',\n",
       "       u'screenName', u'followersCount', u'friendsCount', u'statusesCount',\n",
       "       u'userLocation', u'swiss', u'canton', u'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Special Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_categories = handle_special_categories(english_tweets)\n",
    "len(replaced_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Still the best coffee in town \\u2014 at La Stanza '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_categories['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing contractions (needed for more accurate tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_no_contractions = replace_contractions(replaced_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Still the best coffee in town \\u2014 at La Stanza '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_no_contractions['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tokenization of Tweets into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_list = bag_of_word_representation(tweets_no_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Still', u'the', u'best', u'coffee', u'in', u'town', u'at', u'La', u'Stanza']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Still', 'RB'),\n",
       " (u'the', 'DT'),\n",
       " (u'best', 'JJS'),\n",
       " (u'coffee', 'NN'),\n",
       " (u'in', 'IN'),\n",
       " (u'town', 'NN'),\n",
       " (u'at', 'IN'),\n",
       " (u'La', 'NNP'),\n",
       " (u'Stanza', 'NNP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tweets = pos_tagging(tokenized_list)\n",
    "tagged_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STANFORD VERSION : More accurate but is too slow:\n",
    "#import os\n",
    "#os.environ[\"STANFORD_MODELS\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "#os.envir\n",
    "#on[\"STANFORD_PARSER\"] = \"/home/meryem/Downloads/stanford-parser-full-2016-10-31\"\n",
    "#from nltk.parse.stanford import StanfordDependencyParser\n",
    "#dep_parser=StanfordDependencyParser(model_path=\"/home/meryem/Downloads/stanford-parser-full-2016-10-31/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "#dependency_trees = []\n",
    "#for tweet in tweets_no_contractions['text']:\n",
    "    #result = dep_parser.raw_parse(\"No rest is detrimental\")\n",
    "    #dep = result.next()\n",
    "    #dependency_trees.append(list(dep.triples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMPORARY SOLUTION FOR DEPENDENCY PARSING:\n",
    "# to install Run the following:\n",
    "# pip install -U spacy\n",
    "# python -m spacy.en.download all # for ENGLISH\n",
    "# python -m spacy.de.download all # for Deutch\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "# Joining text:\n",
    "tweets_text = []\n",
    "for i in range(0, len(tokenized_list)):\n",
    "    space = \" \"\n",
    "    tweets_text.append(space.join(tokenized_list[i]))\n",
    "tweets_text[0].encode(\"utf-8\")\n",
    "for i in range(0, len(tweets_text)):\n",
    "    doc = nlp(tweets_text[i])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples = []\n",
    "for sample in docs:\n",
    "    new_samples_sub = []\n",
    "    for word in sample:\n",
    "        new_samples_sub.append((unicode(word),word.pos_))\n",
    "    new_samples.append(new_samples_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Syntactic Rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_samples = apply_syntactic_rules(docs,new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<<< Original tweet text >>>\n",
      "\n",
      "massive miles more in april snowboarding swiss alps usertesting in cali ux intensive in amsterdam excited\n",
      "\n",
      "<<<< Tweet after applying syntactic Rules >>>\n",
      "\n",
      "[u'massive', u'miles', u'more', u'in', u'april', u'snowboarding', u'swiss', u'alps', u'usertesting', u'in', u'cali', u'ux', u'intensive', u'in', u'amsterdam', u'excited']\n"
     ]
    }
   ],
   "source": [
    "i = 18\n",
    "print \"\\n<<<< Original tweet text >>>\\n\"\n",
    "print tweets_text[i]\n",
    "print \"\\n<<<< Tweet after applying syntactic Rules >>>\\n\"\n",
    "#print new_samples[i]\n",
    "new_tweet = []\n",
    "for (word,pos) in new_samples[i]:\n",
    "    new_tweet.append(word)\n",
    "print new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Still', 'RB'), (u'the', 'DT'), (u'best', 'JJS'), (u'coffee', 'NN'), (u'in', 'IN'), (u'town', 'NN'), (u'at', 'IN'), (u'La', 'NNP'), (u'Stanza', 'NNP')]\n",
      "\n",
      "\n",
      "[(u'the', u'DET'), (u'best', u'ADJ'), (u'in', u'ADP'), (u'town', u'NOUN'), (u'at', u'ADP'), (u'La', u'PROPN'), (u'Stanza', u'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "print tagged_tweets[0]\n",
    "print \"\\n\"\n",
    "print new_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_without_ne = remove_named_entities(new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Getting', 'v'), (u'ready', u'ADJ')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tags = normalize_pos_tags_words(tweet_without_ne)\n",
    "normalized_tags[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation and Stop words and Converting to Lower Case and Removal of Other special categories: url, number, username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_tweets_without = eliminate_stop_words_punct(normalized_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'best', u'ADJ'), (u'town', 'n'), (u'la', u'PROPN'), (u'stanza', u'PROPN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tweets = lemmatizer(tagged_tweets_without)\n",
    "\n",
    "lemmatized_tweets_untag = lemmatizer_untagged(tagged_tweets_without)\n",
    "lemmatized_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only NAVA words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nava_tweets = keep_only_nava_words(lemmatized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'best', u'ADJ'), (u'town', 'n'), (u'la', u'PROPN'), (u'stanza', u'PROPN')]\n",
      "[u'best', u'town']\n"
     ]
    }
   ],
   "source": [
    "print lemmatized_tweets[0]\n",
    "print nava_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting NRC Lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon = extractLexicon()\n",
    "word_set = list(set(lexicon['Word']))\n",
    "word_emotional_vectors = []\n",
    "for word in word_set:\n",
    "    word_emotional_vectors.append((word,list(lexicon[lexicon['Word']==word]['Score'])))\n",
    "word_emotional_vectors_dict = dict(word_emotional_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '0', '0', '1', '0', '1', '0', '0', '1']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.head(1)\n",
    "word_emotional_vectors_dict['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    5: 'Negative',\n",
    "    6: 'Positive',\n",
    "    7: 'Sadness',\n",
    "    8: 'Surprise',\n",
    "    9: 'Trust',\n",
    "    10: 'Neutral'\n",
    "}\n",
    "emotion_ids = []\n",
    "for word in word_emotional_vectors_dict.keys():\n",
    "    for i in range(0,len(word_emotional_vectors_dict[word])):\n",
    "        if word_emotional_vectors_dict[word][i] == '1':\n",
    "            emotion_ids.append((word,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3324"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_repres_words_list = [] \n",
    "for i in range(0,10):\n",
    "    emotion_repres_words_list_sub = []\n",
    "    for (word,emotion) in emotion_ids:\n",
    "        if emotion == i:\n",
    "            emotion_repres_words_list_sub.append(word)\n",
    "    emotion_repres_words_list.append(emotion_repres_words_list_sub)\n",
    "len(emotion_repres_words_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon_df = pd.DataFrame()\n",
    "lexicon_df[0] = emotion_repres_words_list[0]\n",
    "for i in range(1,10):\n",
    "    df = pd.DataFrame()\n",
    "    df[i] = emotion_repres_words_list[i]\n",
    "    lexicon_df= pd.concat([lexicon_df,df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon_df = pd.read_csv('lexicon_nrc.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon_df.to_csv('lexicon_nrc.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_lexicon = make_unique_lexicon(lexicon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Semantic Similarity using PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatten_list = [word for sublist in lemmatized_tweets for (word, tag) in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_pmi_dict = calculate_pmi(flatten_list,unique_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_pmi_based = compute_matrix_sentences_list(nava_tweets,lexicon_df, clean_pmi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emo_dict = {\n",
    "    0: 'Anger',\n",
    "    1: 'Anticipation',\n",
    "    2: 'Disgust',\n",
    "    3: 'Fear',\n",
    "    4: 'Joy',\n",
    "    #5: 'Negative',\n",
    "    #6: 'Positive',\n",
    "    5: 'Sadness',\n",
    "    6: 'Surprise',\n",
    "    7: 'Trust',\n",
    "    8: 'Neutral'\n",
    "}\n",
    "sent_dict = {\n",
    "    0: \"Positive\",\n",
    "    1: \"Negative\",\n",
    "    2: \"Neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emotion Recognition\n",
    "sentence_vectors_pmi = compute_sentence_emotion_vectors(emotion_pmi_based)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_pmi)\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentence_vectors_sent_pmi = compute_sentence_sentiment_vectors(emotion_pmi_based)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['Affective Feature Representation'] = lemmatized_tweets\n",
    "english_tweets['Emotion Ids'] = sentence_vectors_pmi\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])\n",
    "english_tweets['Emotionalities'] = emotions\n",
    "english_tweets['Sentiments'] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets_col = english_tweets[['text','Affective Feature Representation','Emotion Ids','Emotionalities','Sentiments']]\n",
    "english_tweets_col.to_csv('PMI_Lexicon_ResultsSampleEnglishData.csv',encoding =\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Emotionalities'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(9)\n",
    "cs=cm.Set1(np.arange(9)/9.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Fine-grained Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Sentiments'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(3)\n",
    "cs=cm.Set1(np.arange(3)/3.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Sentiment Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Same approach using word2vec similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training Word2Vec Model on the whole tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_tokenized_lemma = lemmatizer_raw(tagged_tweets)\n",
    "len(raw_tokenized_lemma)\n",
    "raw_tokenized_lemma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_word2Vec_model(300, 40, 4, 10, 1e-3 , raw_tokenized_lemma, \"geo_tweets_word2vec_model\"): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.similarity('happy','so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_word2vec_based = compute_matrix_sentences_list_word2vec(lemmatized_tweets,lexicon_df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotion_word2vec_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Emotion Recognition\n",
    "sentence_vectors_word2vec = compute_sentence_emotion_vectors(emotion_word2vec_based)\n",
    "\n",
    "emotionalities = compute_emotionalities(sentence_vectors_word2vec)\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentence_vectors_sent_word2vec = compute_sentence_sentiment_vectors(emotion_word2vec_based)\n",
    "\n",
    "sentiments = compute_sentiments(sentence_vectors_sent_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_tweets['Affective Feature Representation'] = lemmatized_tweets\n",
    "english_tweets['Emotion Ids'] = sentence_vectors_pmi\n",
    "emotions = []\n",
    "senti = []\n",
    "for i in range(0,len(emotionalities)):\n",
    "    emotions.append(emo_dict[emotionalities[i]])\n",
    "    senti.append(sent_dict[sentiments[i]])\n",
    "english_tweets['Emotionalities'] = emotions\n",
    "english_tweets['Sentiments'] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_tweets_col = english_tweets[['text','Affective Feature Representation','Emotion Ids','Emotionalities','Sentiments']]\n",
    "english_tweets_col.to_csv('Word2Vec_Lexicon_ResultsSampleEnglishData.csv',encoding =\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Emotionalities'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(9)\n",
    "cs=cm.Set1(np.arange(9)/9.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Fine-grained Emotion Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sentiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "frequency = Counter(english_tweets_col['Sentiments'])\n",
    "df = pd.DataFrame.from_dict(frequency, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "a=np.random.random(3)\n",
    "cs=cm.Set1(np.arange(3)/3.)\n",
    "f=plt.figure()\n",
    "ax=f.add_subplot(111, aspect='equal')\n",
    "patches, texts = plt.pie(df, colors=cs, startangle=90)\n",
    "labels = df.index\n",
    "p=plt.pie(df, colors=cs, labels = labels)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.title(\"PMI- Rule Based Sentiment Emotion Distribution\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
